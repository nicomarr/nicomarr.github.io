{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literate source code of helper functions to update publications page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ~/GitHub/nicomarr.github.io/_data # Contains the actual data files\n",
    "%ls ~/GitHub/nicomarr.github.io/_py/test_data # For testing and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the test data directory\n",
    "data_dir = os.path.expanduser(\"~/GitHub/nicomarr.github.io/_py/test_data\") # uncomment for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the data files\n",
    "pmid_list_path = os.path.join(data_dir, \"PMID-export.txt\")\n",
    "print(pmid_list_path)\n",
    "articles_metadata_path = os.path.join(data_dir, \"articles-metadata.csv\")\n",
    "print(articles_metadata_path)\n",
    "update_log_path = os.path.join(data_dir, \"update-log.json\")\n",
    "print(update_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pmid_list_path) as f:\n",
    "    pmids = f.read().splitlines()\n",
    "print(f\"No. of PMIDs: {len(pmids)}\\n{pmids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_metadata = pd.read_csv(articles_metadata_path, dtype=str)\n",
    "df_articles_metadata[\"publication_date\"] = pd.to_datetime(df_articles_metadata[\"publication_date\"])\n",
    "df_articles_metadata = df_articles_metadata.sort_values(by=\"publication_date\", ascending=False)\n",
    "df_articles_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(update_log_path) as f:\n",
    "    update_log = json.load(f)\n",
    "update_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make call to the OpenAlex API using the `requests` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "id = \"34427831\" # pmid from the list; for testing\n",
    "base_url = \"https://api.openalex.org/works/\"\n",
    "params = {\n",
    "    \"mailto\": os.environ[\"EMAIL\"],\n",
    "    \"select\": \"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "}\n",
    "url = f\"{base_url}pmid:{id}\"\n",
    "\n",
    "verbose = True  # Define verbose or pass it as a parameter\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "    work = response.json()\n",
    "except requests.RequestException as e:\n",
    "    if verbose:\n",
    "        print(f\"An error occurred while making an API call with UID {id}: {e}\")\n",
    "    work = None\n",
    "except json.JSONDecodeError as e:\n",
    "    if verbose:\n",
    "        print(f\"Failed to decode JSON response for UID {id}: {e}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "    work = None\n",
    "\n",
    "if work is None:\n",
    "    print(f\"Status code: {response.status_code}\")\n",
    "    print(\"Response headers:\")\n",
    "    pprint(dict(response.headers), indent=2)\n",
    "    print(\"Response content:\")\n",
    "    try:\n",
    "        pprint(response.json(), indent=2)\n",
    "    except json.JSONDecodeError:\n",
    "        print(response.text)\n",
    "else:\n",
    "    pprint(work, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `openalex_api_utils` to get metadata from the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.expanduser(\"../utils\"))\n",
    "from openalex_api_utils import get_works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get metadata for a single entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works, failed_calls = get_works(\n",
    "    ids=[\"34427831\"], \n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print relevant metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(works[0][\"metadata\"][\"title\"])\n",
    "    print(works[0][\"metadata\"][\"authorships\"][0][\"author\"][\"display_name\"])\n",
    "    print(works[0][\"metadata\"][\"publication_date\"])\n",
    "    print(works[0][\"metadata\"][\"publication_year\"])\n",
    "    print(works[0][\"metadata\"][\"doi\"])\n",
    "    print(works[0][\"metadata\"][\"ids\"].get(\"pmid\").split(\"/\")[-1]) # PMID\n",
    "    print(works[0][\"metadata\"][\"ids\"].get(\"pmcid\").split(\"/\")[-1]) # PMCID\n",
    "    print(works[0][\"metadata\"][\"id\"]) # OpenAlex ID\n",
    "    print(works[0][\"metadata\"].get(\"best_oa_location\").get(\"pdf_url\")) # may be None\n",
    "    print(works[0][\"metadata\"][\"cited_by_count\"])\n",
    "    print(works[0][\"metadata\"][\"cited_by_api_url\"])\n",
    "    print(works[0][\"metadata\"].get(\"type\"))\n",
    "    print(works[0][\"metadata\"].get(\"type_crossref\"))\n",
    "    print(works[0][\"metadata\"][\"updated_date\"])\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get metadata from multiple entries and iterate over a list of works to extract relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works, failed_calls = get_works( \n",
    "    ids=pmids,\n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    show_progress=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(works), len(failed_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant data from the API response\n",
    "oa_data = []\n",
    "for work in works:\n",
    "    metadata = work[\"metadata\"]\n",
    "    first_author_last_name = metadata[\"authorships\"][0][\"author\"][\"display_name\"].split(\" \")[-1]\n",
    "    article_title = metadata[\"title\"]\n",
    "    journal = metadata[\"primary_location\"][\"source\"][\"display_name\"]\n",
    "    publication_year = str(metadata[\"publication_year\"])\n",
    "    publication_date = metadata[\"publication_date\"]\n",
    "    pmid = metadata[\"ids\"].get(\"pmid\", \"\").split(\"/\")[-1] # remove the url prefix\n",
    "    pmcid = metadata[\"ids\"].get(\"pmcid\")\n",
    "    if pmcid is not None:\n",
    "        pmcid = pmcid.split(\"/\")[-1] # remove the url prefix\n",
    "    else: \n",
    "        pmcid = \"NaN\"\n",
    "    oaid = metadata[\"id\"]\n",
    "    try:\n",
    "        pdf_url = metadata.get(\"best_oa_location\", {}).get(\"pdf_url\", \"not available\")\n",
    "    except AttributeError:\n",
    "        pdf_url = \"not available\"\n",
    "    if pdf_url is None:\n",
    "        pdf_url = \"not available\"\n",
    "    doi_url = metadata[\"doi\"]\n",
    "    cited_by_count = str(metadata[\"cited_by_count\"])\n",
    "    cited_by_ui_url = metadata[\"cited_by_api_url\"].replace(\"api.openalex.org\", \"openalex.org\")\n",
    "    type = metadata.get(\"type\")\n",
    "    type_crossref = metadata.get(\"type_crossref\")\n",
    "    updated_date = metadata.get(\"updated_date\")\n",
    "\n",
    "    # Append the extracted data to the list\n",
    "    oa_data.append([\n",
    "        first_author_last_name, article_title, journal, publication_year,\n",
    "        publication_date, pmid, pmcid, oaid, pdf_url,\n",
    "        doi_url, cited_by_count, cited_by_ui_url, type, type_crossref, updated_date\n",
    "    ])\n",
    "print(f\"{len(oa_data)} entries:\")\n",
    "print(json.dumps(oa_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the specified columns\n",
    "columns = [\n",
    "    'first_author_last_name', 'article_title', 'journal',\n",
    "    'publication_year', 'publication_date', 'pmid', 'pmcid', 'oaid',\n",
    "    'pdf_url', 'doi_url', 'cited_by_count', 'cited_by_ui_url', 'type',\n",
    "    'type_crossref', 'updated_date'\n",
    "]\n",
    "\n",
    "df_works = pd.DataFrame(oa_data, columns=columns)\n",
    "df_works = df_works.drop_duplicates(subset=[\"pmid\"])\n",
    "df_works = df_works[df_works[\"type\"] != \"erratum\"]\n",
    "df_works.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function that parses metadata and returns the relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_data(works: List[Dict[str, Any]], exclude_errata = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the raw data from the OpenAlex API and create a DataFrame.\n",
    "\n",
    "    This function extracts relevant information from each work in the input list\n",
    "    and creates a DataFrame with specified columns. It also removes duplicates\n",
    "    based on PMID and filters out errata (if specified).\n",
    "\n",
    "    Args:\n",
    "        works (List[Dict[str, Any]]): A list of dictionaries, where each dictionary\n",
    "            contains metadata about a work.\n",
    "        exclude_errata (bool): Whether to exclude errata from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing extracted and processed information\n",
    "        from the works.\n",
    "\n",
    "    Example:\n",
    "        >>> df_works = create_works_dataframe(works)\n",
    "        >>> df_works.head()\n",
    "\n",
    "    Note:\n",
    "        The function extracts the following information for each work:\n",
    "        - First author's last name\n",
    "        - Article title\n",
    "        - Journal name\n",
    "        - Publication year and date\n",
    "        - PMID, PMCID, and OpenAlex ID\n",
    "        - PDF URL (if available)\n",
    "        - DOI URL\n",
    "        - Citation count and URL\n",
    "        - Work type and Crossref type\n",
    "        - Updated date (from the API)\n",
    "    \"\"\"\n",
    "    oa_data = []\n",
    "    for work in works:\n",
    "        metadata = work[\"metadata\"]\n",
    "        first_author_last_name = metadata[\"authorships\"][0][\"author\"][\"display_name\"].split(\" \")[-1]\n",
    "        article_title = metadata[\"title\"]\n",
    "        journal = metadata[\"primary_location\"][\"source\"][\"display_name\"]\n",
    "        publication_year = str(metadata[\"publication_year\"])\n",
    "        publication_date = metadata[\"publication_date\"]\n",
    "        pmid = metadata[\"ids\"].get(\"pmid\", \"\").split(\"/\")[-1] # remove the url prefix\n",
    "        pmcid = metadata[\"ids\"].get(\"pmcid\")\n",
    "        if pmcid is not None:\n",
    "            pmcid = pmcid.split(\"/\")[-1] # remove the url prefix\n",
    "        else: \n",
    "            pmcid = \"NaN\"\n",
    "        oaid = metadata[\"id\"]\n",
    "        try:\n",
    "            pdf_url = metadata.get(\"best_oa_location\", {}).get(\"pdf_url\", \"not available\")\n",
    "        except AttributeError:\n",
    "            pdf_url = \"not available\"\n",
    "        if pdf_url is None:\n",
    "            pdf_url = \"not available\"\n",
    "        doi_url = metadata[\"doi\"]\n",
    "        cited_by_count = str(metadata[\"cited_by_count\"])\n",
    "        cited_by_ui_url = metadata[\"cited_by_api_url\"].replace(\"api.openalex.org\", \"openalex.org\")\n",
    "        work_type = metadata.get(\"type\")\n",
    "        type_crossref = metadata.get(\"type_crossref\")\n",
    "        updated_date = metadata.get(\"updated_date\")\n",
    "\n",
    "        oa_data.append([\n",
    "            first_author_last_name, article_title, journal, publication_year,\n",
    "            publication_date, pmid, pmcid, oaid, pdf_url, doi_url,\n",
    "            cited_by_count, cited_by_ui_url, work_type, type_crossref, updated_date\n",
    "        ])\n",
    "\n",
    "    columns = [\n",
    "        'first_author_last_name', 'article_title', 'journal',\n",
    "        'publication_year', 'publication_date', 'pmid', 'pmcid', 'oaid',\n",
    "        'pdf_url', 'doi_url', 'cited_by_count', 'cited_by_ui_url', 'type',\n",
    "        'type_crossref', 'updated_date'\n",
    "    ]\n",
    "\n",
    "    df_works = pd.DataFrame(oa_data, columns=columns, dtype=str)\n",
    "    df_works = df_works.drop_duplicates(subset=[\"pmid\"])\n",
    "    if exclude_errata:\n",
    "        df_works = df_works[df_works[\"type\"] != \"erratum\"]\n",
    "\n",
    "    df_works[\"publication_date\"] = pd.to_datetime(df_works[\"publication_date\"], format='%Y-%m-%d', errors='coerce')\n",
    "    df_works = df_works.sort_values(by=\"publication_date\", ascending=False)\n",
    "    df_works.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_works = parse_data(works)\n",
    "df_works.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare local data with external data and find mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the shapes of the DataFrames\n",
    "print(\"df_works shape:\", df_works.shape)\n",
    "print(\"df_articles_metadata shape:\", df_articles_metadata.shape)\n",
    "assert df_works.shape == df_articles_metadata.shape, \"DataFrames have different shapes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the column names:\n",
    "print(\"df_works columns:\", df_works.columns.tolist())\n",
    "print(\"articles_metadata columns:\", df_articles_metadata.columns.tolist())\n",
    "assert df_works.columns.tolist() == df_articles_metadata.columns.tolist(), \"Column names are different.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common columns:\n",
    "common_columns = set(df_works.columns) & set(df_articles_metadata.columns)\n",
    "print(\"Common columns:\", common_columns)\n",
    "print(\"No. of common columns:\", len(common_columns))\n",
    "\n",
    "# Check for columns only in one of the DataFrames:\n",
    "works_columns = set(df_works.columns)\n",
    "articles_metadata_columns = set(df_articles_metadata.columns)\n",
    "print(\"Columns only in df_works:\", works_columns - articles_metadata_columns)\n",
    "print(\"Columns only in articles_metadata:\", articles_metadata_columns - works_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the datatypes:\n",
    "common_columns = list(common_columns)\n",
    "df_works_dtypes = df_works[common_columns].dtypes\n",
    "articles_metadata_dtypes = df_articles_metadata[common_columns].dtypes\n",
    "dtypes_comparison = df_works_dtypes == articles_metadata_dtypes\n",
    "\n",
    "print(\"Columns with mismatched datatypes:\")\n",
    "for column in common_columns:\n",
    "    if df_works_dtypes[column] == articles_metadata_dtypes[column]:\n",
    "        continue\n",
    "    print(f\"{column}:\")\n",
    "    print(f\"  df_works: {df_works_dtypes[column]}\")\n",
    "    print(f\"  articles_metadata: {articles_metadata_dtypes[column]}\")\n",
    "    print(f\"  Match: {dtypes_comparison[column]}\")\n",
    "    print()\n",
    "\n",
    "# leave pmid dtypes as int for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the \"pmid\" values:\n",
    "df_works_pmids = set(df_works[\"pmid\"])\n",
    "articles_metadata_pmids = set(df_articles_metadata[\"pmid\"].astype(str))\n",
    "common_pmids = df_works_pmids & articles_metadata_pmids\n",
    "\n",
    "print(\"Number of common PMIDs:\", len(common_pmids))\n",
    "print(\"PMIDs only in df_works:\", len(df_works_pmids - articles_metadata_pmids))\n",
    "print(\"PMIDs only in articles_metadata:\", len(articles_metadata_pmids - df_works_pmids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the values for article_title in both DataFrames\n",
    "series1 = df_works[\"article_title\"].reset_index(drop=True)\n",
    "series2 = df_articles_metadata[\"article_title\"].reset_index(drop=True)\n",
    "title_mismatch = series1.compare(series2)\n",
    "title_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the values for pdf_url in both DataFrames\n",
    "series1 = df_works[\"pdf_url\"].reset_index(drop=True)\n",
    "series2 = df_articles_metadata[\"pdf_url\"].reset_index(drop=True)\n",
    "pdf_url_mismatch = series1.compare(series2)\n",
    "pdf_url_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the values for the doi_url in both DataFrames\n",
    "series1 = df_works[\"doi_url\"].reset_index(drop=True)\n",
    "series2 = df_articles_metadata[\"doi_url\"].reset_index(drop=True)\n",
    "pdf_url_mismatch = series1.compare(series2)\n",
    "pdf_url_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all columns\n",
    "result = df_works.reset_index(drop=True).compare(df_articles_metadata.reset_index(drop=True))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to add metadata of new articles to the local/existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids.append(\"39198650\") # add a new PMID for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find IDs in the PMID list missing from the local metadata\n",
    "new_pmids = set(pmids) - set(df_articles_metadata[\"pmid\"].astype(str))\n",
    "new_pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make API calls to get the missing PMIDs\n",
    "exclude_errata=True\n",
    "new_works, failed_calls = get_works(\n",
    "    ids=list(new_pmids),\n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "df_new_works = parse_data(new_works, exclude_errata=exclude_errata)\n",
    "if len(df_new_works) == 0:\n",
    "    print(\"No new articles found.\")\n",
    "    print(f\"Failed calls: {len(failed_calls)}\")\n",
    "    print(f\"Errata excluded: {exclude_errata}\")\n",
    "    print(f\"No. of errata in new works: {len(df_new_works[df_new_works['type'] == 'erratum'])}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n{len(df_new_works)} new article(s) found.\\n\")\n",
    "    print(df_new_works[[\"article_title\"]])\n",
    "assert len(df_new_works) == 1, \"Number of new articles is not match the expected number.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_new_works) == 0:\n",
    "    print(\"No new articles found.\")\n",
    "else:\n",
    "    df_updated_works = pd.concat([df_works, df_new_works], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated_works = df_updated_works.sort_values(by=\"publication_date\", ascending=False)\n",
    "assert df_new_works.at[0, 'article_title'] in df_updated_works[\"article_title\"].values\n",
    "df_updated_works.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from typing import Tuple, Set, List, Dict, Any\n",
    "\n",
    "def append_metadata(metadata_file_path: str, pmid_file_path: str, exclude_errata: bool = True, verbose: bool = True) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Append metadata for missing PMIDs to an existing metadata file.\n",
    "\n",
    "    Args:\n",
    "        metadata_file_path (str): Path to CSV file containing existing metadata.\n",
    "        pmid_file_path (str): Path to file containing list of PMIDs.\n",
    "        verbose (bool): Whether to show verbose messages during the process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a boolean indicating if any updates were made, and a message string with details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert metadata_file_path.endswith(\".csv\"), \"Invalid file format. Please provide a CSV file.\"\n",
    "    assert os.path.exists(metadata_file_path), \"Metadata file not found.\"\n",
    "    assert pmid_file_path.endswith(\".txt\"), \"Invalid file format. Please provide a TXT file.\"\n",
    "    assert os.path.exists(pmid_file_path), \"PMID file not found.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean.\"\n",
    "\n",
    "    # Read existing metadata\n",
    "    if verbose: print(\"Reading the existing metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(metadata_file_path, dtype=str)\n",
    "        metadata_bkp = deepcopy(metadata) # Make a deepcopy of the DataFrame to save a backup\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the metadata file: {e}\")\n",
    "        return False, f\"An error occurred while reading the metadata file: {e}\"\n",
    "\n",
    "    # Read PMIDs from file\n",
    "    if verbose: print(\"Reading the PMID file...\")\n",
    "    try:\n",
    "        with open(pmid_file_path, 'r') as f:\n",
    "            pmids = set(line.strip() for line in f)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the PMID file: {e}\")\n",
    "        return False, f\"An error occurred while reading the PMID file: {e}\"\n",
    "\n",
    "    # Find missing PMIDs\n",
    "    if verbose: print(\"Searching for new PMIDs not in the metadata...\")\n",
    "    existing_pmids: Set[str] = set(metadata[\"pmid\"])\n",
    "    new_pmids: Set[str] = pmids - existing_pmids\n",
    "    new_pmids_str = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "    if verbose: print(f\"Found {len(new_pmids)} new PMID(s): {new_pmids_str}.\")\n",
    "    if len(new_pmids) == 0:\n",
    "        return False, \"No new PMIDs found.\"\n",
    "    else:\n",
    "        try:\n",
    "            # Make API calls to get the missing PMIDs\n",
    "            exclude_errata: bool = True\n",
    "            select_fields: str = (\n",
    "                \"id,title,doi,primary_location,authorships,publication_year,\"\n",
    "                \"publication_date,ids,best_oa_location,cited_by_count,\"\n",
    "                \"cited_by_api_url,type,type_crossref,updated_date\"\n",
    "            )\n",
    "            new_articles, failed_calls = get_works(\n",
    "                ids=list(new_pmids),\n",
    "                email=os.environ.get(\"EMAIL\"),\n",
    "                select_fields=select_fields,\n",
    "                show_progress=verbose\n",
    "            )\n",
    "            if verbose: print(f\"API calls completed. Failed calls: {len(failed_calls)}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while fetching works data from the API: {e}\")\n",
    "            return False, f\"An error occurred while fetching works data from the API: {e}\"\n",
    "\n",
    "        # Parse the data for new articles\n",
    "        if verbose: print(\"Parsing the data for new articles...\")\n",
    "        try:\n",
    "            df_new_articles = parse_data(new_articles, exclude_errata=True)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while parsing the data for new articles: {e}\")\n",
    "            return False, f\"An error occurred while parsing the data for new articles: {e}\"\n",
    "        \n",
    "        if df_new_articles.empty:\n",
    "            if exclude_errata:\n",
    "                if verbose: print(\"No new articles found (Errata excluded).\")\n",
    "                return False, \"No new articles found (Errata excluded).\"\n",
    "            else:\n",
    "                if verbose: print(\"No new articles found.\")\n",
    "                return False, \"No new articles found.\"\n",
    "        else:\n",
    "            # Append the new articles to the existing metadata\n",
    "            new_pmids = set(df_new_articles[\"pmid\"])\n",
    "            new_pmids = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "            if verbose: print(f\"Appending {len(df_new_articles)} new article(s) with PMID(s) {new_pmids} to the existing metadata...\")\n",
    "            try:\n",
    "                metadata = pd.concat([metadata, df_new_articles], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"An error occurred while appending the new articles to the existing metadata: {e}\")\n",
    "                return False, f\"An error occurred while appending the new articles to the existing metadata: {e}\"\n",
    "\n",
    "            # Save the updated metadata to a CSV file\n",
    "            if verbose: print(\"Saving the updated metadata to a CSV file...\")\n",
    "            metadata.to_csv(metadata_file_path, index=False)\n",
    "            if verbose: print(\"Saving a backup file to disk...\")\n",
    "            bkp_file_path = metadata_file_path.replace(\".csv\", f\"_bkp-{datetime.now().strftime('%Y%m%d-%Hh%Mm')}.csv\")\n",
    "            metadata_bkp.to_csv(bkp_file_path, index=False)\n",
    "            if verbose: print(\"Metadata updated successfully.\")\n",
    "\n",
    "            # Update the log file\n",
    "            log_file_path = os.path.join(os.path.dirname(metadata_file_path), \"update-log.json\")\n",
    "            try:\n",
    "                # get the path to the log file from the metadata file path\n",
    "                if verbose: print(\"Updating the log file...\")\n",
    "                with open(log_file_path, \"r\") as f:\n",
    "                    update_log = json.load(f)\n",
    "                # format {\"last_modified\": \"2024-08-06\"}\n",
    "                update_log[\"last_modified\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump(update_log, f)\n",
    "                if verbose: print(f\"Log file updated successfully.\")\n",
    "            except Exception as e: # Note: Additional error handling has been added in the final version below\n",
    "                if verbose: print(f\"No log file found. Error: {e}. Creating a new log file...\")\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump({\"last_modified\": datetime.now().strftime(\"%Y-%m-%d\")}, f)\n",
    "                if verbose: print(f\"Log file created successfully.\")\n",
    "\n",
    "            return True, f\"Appended {len(df_new_articles)} article(s) and saved file to {metadata_file_path}. Backup saved as {bkp_file_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to update citation counts of all articles in the local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get up-to-date metadata for articles in the local metadata file; only cited_by_count and updated_date will be updated\n",
    "works, failed_calls = get_works( \n",
    "    ids=df_articles_metadata[\"pmid\"].astype(str).tolist(),\n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,doi,title,cited_by_count,updated_date\",\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the citation counts\n",
    "# for work in works:\n",
    "#     print(f\"{work[\"metadata\"][\"title\"][:30]}... {work[\"metadata\"][\"doi\"].replace(\"https://doi.org/\",\"\")}, Citation count: {work[\"metadata\"][\"cited_by_count\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset a row in df_articles_metadata based on a condition\n",
    "df_articles_metadata[df_articles_metadata[\"doi_url\"] == \"https://doi.org/10.1038/s41586-024-07745-x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the rows in df_articles_metadata and update the cited_by_count and updated_date\n",
    "counter = 0\n",
    "\n",
    "# make a deepcopy of the DataFrame to save a backup\n",
    "from copy import deepcopy\n",
    "df_articles_metadata_bkp = deepcopy(df_articles_metadata)\n",
    "\n",
    "for index, row in df_articles_metadata.iterrows():\n",
    "    id = row[\"oaid\"]\n",
    "    current_cited_by_count = row[\"cited_by_count\"]\n",
    "    work = next((work for work in works if work[\"metadata\"][\"id\"] == id), None)\n",
    "    new_cited_by_count = work[\"metadata\"][\"cited_by_count\"]\n",
    "    if new_cited_by_count > int(current_cited_by_count):\n",
    "        try:\n",
    "            print(f\"Updating the cited_by_count for ID: {id.split('/')[-1]} from {current_cited_by_count} to {new_cited_by_count}\")\n",
    "            df_articles_metadata.at[index, \"cited_by_count\"] = new_cited_by_count\n",
    "            df_articles_metadata.at[index, \"updated_date\"] = work[\"metadata\"][\"updated_date\"]\n",
    "            counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to update the cited_by_count for PMID: {pmid}\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(f\"Citation count for ID: {id.split('/')[-1]} is up-to-date. Citattion count: {current_cited_by_count}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "print(f\"Updated values for {counter} articles.\")\n",
    "if counter > 0:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(os.path.expanduser(\"../utils\"))\n",
    "from openalex_api_utils import get_works\n",
    "import json\n",
    "\n",
    "def update_citations(file_path: str, verbose: bool = True) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Update the citation counts in the articles metadata file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the articles metadata file.\n",
    "        verbose (bool): Whether to show verbose messages the process, including progress and errors.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a boolean indicating if any updates were made, and a message string with details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert file_path.endswith(\".csv\"), \"Invalid file format. Please provide a CSV file.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean.\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return False\n",
    "\n",
    "    # Read the metadata file and sort by publication date, descending\n",
    "    if verbose: print(\"Reading the metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(file_path, dtype=str)\n",
    "        metadata_bkp = deepcopy(metadata) # Make a deepcopy of the DataFrame to save a backup\n",
    "        metadata[\"publication_date\"] = pd.to_datetime(metadata[\"publication_date\"]) # Note: Date parsing has been modified in the final version\n",
    "        metadata = metadata.sort_values(by=\"publication_date\", ascending=False)\n",
    "        metadata.reset_index(drop=True, inplace=True)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the metadata file: {e}\")\n",
    "        return False, f\"An error occurred while reading the metadata file: {e}\"\n",
    "    \n",
    "    # Extract PMIDs from the metadata\n",
    "    if verbose: print(\"Extracting PMIDs from the metadata...\")\n",
    "    try:\n",
    "        pmids = metadata[\"pmid\"].astype(str).tolist()\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while extracting PMIDs from the metadata: {e}\")\n",
    "        return False, f\"An error occurred while extracting PMIDs from the metadata: {e}\"\n",
    "    \n",
    "    # Make API calls to get the works for the PMIDs\n",
    "    if verbose: print(\"Fetching works data from the API...\")\n",
    "    works, failed_calls = get_works(\n",
    "        ids=pmids,\n",
    "        email=os.environ.get(\"EMAIL\"), # Will return an empty string if the variable is not set\n",
    "        select_fields=\"id,doi,title,cited_by_count,updated_date\",\n",
    "        show_progress=verbose\n",
    "    )\n",
    "    \n",
    "    # Iterate over the rows in metadata and update the cited_by_count and updated_date\n",
    "    if verbose: print(\"Updating the citation counts...\")\n",
    "    counter = 0\n",
    "    for index, row in metadata.iterrows():\n",
    "        id = row[\"oaid\"]\n",
    "        doi = row[\"doi_url\"]\n",
    "        updated_date = row[\"updated_date\"]\n",
    "        current_cited_by_count = row[\"cited_by_count\"]\n",
    "        work = next((work for work in works if work[\"metadata\"][\"id\"] == id), None)\n",
    "        new_cited_by_count = work[\"metadata\"].get(\"cited_by_count\")\n",
    "        if new_cited_by_count > int(current_cited_by_count):\n",
    "            try:\n",
    "                if verbose: print(f\"Updating the cited_by_count for ID: {id} / DOI: {doi} from {current_cited_by_count} to {new_cited_by_count}\")\n",
    "                metadata.at[index, \"cited_by_count\"] = new_cited_by_count\n",
    "                metadata.at[index, \"updated_date\"] = work[\"metadata\"][\"updated_date\"]\n",
    "                counter += 1\n",
    "            except Exception as e:\n",
    "                if verbose: print(f\"Failed to update the cited_by_count for PMID: {pmid}. Error: {e}\")\n",
    "        else:\n",
    "            if verbose: print(f\"Citation count {current_cited_by_count} for ID {id} with DOI {doi} is up-to-date. Skipping...\")\n",
    "            continue\n",
    "\n",
    "    if verbose: print(f\"Updated values for {counter} articles.\")\n",
    "    if counter > 0:\n",
    "        if verbose: print(\"Saving the updated metadata to a CSV file...\")\n",
    "        metadata.to_csv(file_path, index=False)\n",
    "        if verbose: print(\"Saving a backup file to disk...\")\n",
    "        bkp_file_path = file_path.replace(\".csv\", f\"_bkp-{datetime.now().strftime('%Y%m%d-%Hh%Mm')}.csv\")\n",
    "        metadata_bkp.to_csv(bkp_file_path, index=False)\n",
    "        if verbose: print(\"Metadata updated successfully.\")\n",
    "\n",
    "        # Update the log file\n",
    "        try:\n",
    "            # get the path to the log file from the metadata file path\n",
    "            if verbose: print(\"Updating the log file...\")\n",
    "            log_file_path = os.path.join(os.path.dirname(file_path), \"update-log.json\")\n",
    "            with open(log_file_path, \"r\") as f:\n",
    "                update_log = json.load(f)\n",
    "            # format {\"last_modified\": \"2024-08-06\"}\n",
    "            update_log[\"last_modified\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            with open(log_file_path, \"w\") as f:\n",
    "                json.dump(update_log, f)\n",
    "            if verbose: print(f\"Log file updated successfully.\")\n",
    "        except Exception as e: # Note: Additional error handling has been added in the final version below\n",
    "            if verbose: print(f\"No log file found. Error: {e}. Creating a new log file...\")\n",
    "            with open(log_file_path, \"w\") as f:\n",
    "                json.dump({\"last_modified\": datetime.now().strftime(\"%Y-%m-%d\")}, f)\n",
    "            if verbose: print(f\"Log file created successfully.\")\n",
    "\n",
    "        return True, f\"Updated values for {counter} articles and saved file to {file_path}. Backup saved as {bkp_file_path}\"\n",
    "    else:\n",
    "        return False, \"Loaded metadata were up-to-date. No changes were made.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~/GitHub/nicomarr.github.io/_py/test_data\") # uncomment for local testing\n",
    "test_file_path = os.path.join(data_dir, \"articles-metadata_copy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_citations(test_file_path, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate module from the above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../utils/__init__.py\n",
    "__version__ = \"0.0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../utils/website_utils.py\n",
    "import os\n",
    "from typing import Tuple, Set, List, Dict, Any\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from openalex_api_utils import get_works\n",
    "import json\n",
    "\n",
    "def update_citations(file_path: str, verbose: bool = True) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Update the citation counts in the articles metadata file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the articles metadata file.\n",
    "        verbose (bool): Whether to show verbose messages during the process, including progress and errors.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a boolean indicating if any updates were made, and a message string with details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert file_path.endswith(\".csv\"), \"Invalid file format. Please provide a CSV file.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean.\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return False, f\"File not found: {file_path}\"\n",
    "\n",
    "    # Read the metadata file and sort by publication date, descending\n",
    "    if verbose: print(\"Reading the metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(file_path, dtype=str)\n",
    "        metadata_bkp = deepcopy(metadata) # Make a deepcopy of the DataFrame to save a backup\n",
    "        metadata['publication_date'] = pd.to_datetime(metadata['publication_date'], format='%Y-%m-%d', errors='coerce') # The errors='coerce' parameter will replace any unparsable dates with NaT (Not a Time) values\n",
    "        if metadata['publication_date'].isna().any(): # Check if there are any NaT values\n",
    "            if verbose:\n",
    "                print(\"Warning: Some publication dates could not be parsed. These will be excluded from sorting.\")\n",
    "                entries_with_missing_dates = metadata[metadata['publication_date'].isna()]\n",
    "                print(\n",
    "                    f\"Entries with publication dates that could not be parsed: idx {entries_with_missing_dates.index.tolist()}, \"\n",
    "                    f\"PMIDs {entries_with_missing_dates['pmid'].tolist()}, \"\n",
    "                    f\"Article titles {entries_with_missing_dates['article_title'].tolist()}\"\n",
    "                    )\n",
    "            metadata = metadata.dropna(subset=['publication_date']) # Drop rows with missing publication dates\n",
    "        metadata = metadata.sort_values(by=\"publication_date\", ascending=False)\n",
    "        metadata.reset_index(drop=True, inplace=True)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the metadata file: {e}\")\n",
    "        return False, f\"An error occurred while reading the metadata file: {e}\"\n",
    "    \n",
    "    # Extract PMIDs from the metadata\n",
    "    if verbose: print(\"Extracting PMIDs from the metadata...\")\n",
    "    try:\n",
    "        pmids = metadata[\"pmid\"].astype(str).tolist()\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while extracting PMIDs from the metadata: {e}\")\n",
    "        return False, f\"An error occurred while extracting PMIDs from the metadata: {e}\"\n",
    "    \n",
    "    # Make API calls to get the works for the PMIDs\n",
    "    if verbose: print(\"Fetching works data from the API...\")\n",
    "    works, failed_calls = get_works(\n",
    "        ids=pmids,\n",
    "        email=os.environ.get(\"EMAIL\"), # Will return an empty string if the variable is not set\n",
    "        select_fields=\"id,doi,title,cited_by_count,updated_date\",\n",
    "        show_progress=verbose\n",
    "    )\n",
    "    \n",
    "    # Iterate over the rows in metadata and update the cited_by_count and updated_date\n",
    "    if verbose: print(\"Updating the citation counts...\")\n",
    "    counter = 0\n",
    "    for index, row in metadata.iterrows():\n",
    "        id = row[\"oaid\"]\n",
    "        doi = row[\"doi_url\"]\n",
    "        updated_date = row[\"updated_date\"]\n",
    "        current_cited_by_count = row[\"cited_by_count\"]\n",
    "        work = next((work for work in works if work[\"metadata\"][\"id\"] == id), None)\n",
    "        new_cited_by_count = work[\"metadata\"][\"cited_by_count\"]\n",
    "        if new_cited_by_count > int(current_cited_by_count):\n",
    "            try:\n",
    "                if verbose: print(f\"Updating the cited_by_count for ID: {id} / DOI: {doi} from {current_cited_by_count} to {new_cited_by_count}\")\n",
    "                metadata.at[index, \"cited_by_count\"] = new_cited_by_count\n",
    "                metadata.at[index, \"updated_date\"] = work[\"metadata\"][\"updated_date\"] # Leave the date as a string\n",
    "                counter += 1\n",
    "            except Exception as e:\n",
    "                if verbose: print(f\"Failed to update the cited_by_count for PMID: {pmid}. Error: {e}\")\n",
    "        else:\n",
    "            if verbose: print(f\"Citation count {current_cited_by_count} for ID {id} with DOI {doi} is up-to-date. Skipping...\")\n",
    "            continue\n",
    "\n",
    "    if verbose: print(f\"Updated values for {counter} articles.\")\n",
    "    if counter > 0:\n",
    "        if verbose: print(\"Saving the updated metadata to a CSV file...\")\n",
    "        metadata.to_csv(file_path, index=False)\n",
    "        if verbose: print(\"Saving a backup file to disk...\")\n",
    "        bkp_file_path = file_path.replace(\".csv\", f\"_bkp-{datetime.now().strftime('%Y%m%d-%Hh%Mm')}.csv\")\n",
    "        metadata_bkp.to_csv(bkp_file_path, index=False)\n",
    "        if verbose: print(\"Metadata updated successfully.\")\n",
    "\n",
    "        # Get the path to the log file from the metadata file path\n",
    "        log_file_path = os.path.join(os.path.dirname(file_path), \"update-log.json\") \n",
    "        \n",
    "        # Update the log file\n",
    "        try:\n",
    "            if verbose: print(\"Updating the log file...\")\n",
    "            with open(log_file_path, \"r\") as f:\n",
    "                update_log = json.load(f)\n",
    "            current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            update_log[\"last_modified\"] = current_date # Expected format: {\"last_modified\": \"2024-08-06\"}\n",
    "            with open(log_file_path, \"w\") as f:\n",
    "                json.dump(update_log, f)\n",
    "            if verbose: print(f\"Log file updated successfully.\")\n",
    "        except Exception as e:\n",
    "            if verbose: print(f\"Error updating log file: {e}. Creating a new log file...\")\n",
    "            current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            with open(log_file_path, \"w\") as f:\n",
    "                json.dump({\"last_modified\": current_date}, f)\n",
    "            if verbose: print(f\"New log file created successfully.\")\n",
    "\n",
    "        return True, f\"Updated values for {counter} articles and saved file to {file_path}. Backup saved as {bkp_file_path}\"\n",
    "    else:\n",
    "        return False, \"Loaded metadata were up-to-date. No changes were made.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../utils/website_utils.py\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_data(works: List[Dict[str, Any]], exclude_errata: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the raw data from the OpenAlex API and create a DataFrame.\n",
    "\n",
    "    This function extracts relevant information from each work in the input list\n",
    "    and creates a DataFrame with specified columns. It also removes duplicates\n",
    "    based on PMID and filters out errata (if specified).\n",
    "\n",
    "    Args:\n",
    "        works (List[Dict[str, Any]]): A list of dictionaries, where each dictionary\n",
    "            contains metadata about a work.\n",
    "        exclude_errata (bool): Whether to exclude errata from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing extracted and processed information\n",
    "        from the works.\n",
    "\n",
    "    Example:\n",
    "        >>> df_works = parse_data(works)\n",
    "        >>> df_works.head()\n",
    "\n",
    "    Note:\n",
    "        The function extracts the following information for each work:\n",
    "        - First author's last name\n",
    "        - Article title\n",
    "        - Journal name\n",
    "        - Publication year and date\n",
    "        - PMID, PMCID, and OpenAlex ID\n",
    "        - PDF URL (if available)\n",
    "        - DOI URL\n",
    "        - Citation count and URL\n",
    "        - Work type and Crossref type\n",
    "        - Updated date (from the API)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the extracted data, and iterate over the works data to extract relevant information\n",
    "    oa_data = []\n",
    "    for work in works:\n",
    "        metadata = work[\"metadata\"]\n",
    "        first_author_last_name = metadata[\"authorships\"][0][\"author\"][\"display_name\"].split(\" \")[-1]\n",
    "        article_title = metadata[\"title\"]\n",
    "        journal = metadata[\"primary_location\"][\"source\"][\"display_name\"]\n",
    "        publication_year = str(metadata[\"publication_year\"])\n",
    "        publication_date = metadata[\"publication_date\"]\n",
    "        if publication_date:\n",
    "            try:\n",
    "                publication_date = pd.to_datetime(publication_date).strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                pass # If the date can't be parsed, keep the original string\n",
    "        pmid = metadata[\"ids\"].get(\"pmid\", \"\").split(\"/\")[-1] # To remove the url prefix\n",
    "        pmcid = metadata[\"ids\"].get(\"pmcid\")\n",
    "        if pmcid is not None:\n",
    "            pmcid = pmcid.split(\"/\")[-1] # To remove the url prefix\n",
    "        else: \n",
    "            pmcid = \"\" # To replace None with an empty string\n",
    "        oaid = metadata[\"id\"]\n",
    "        try:\n",
    "            pdf_url = metadata.get(\"best_oa_location\", {}).get(\"pdf_url\", \"not available\")\n",
    "        except AttributeError:\n",
    "            pdf_url = \"not available\"\n",
    "        if pdf_url is None:\n",
    "            pdf_url = \"not available\"\n",
    "        doi_url = metadata[\"doi\"]\n",
    "        cited_by_count = str(metadata[\"cited_by_count\"])\n",
    "        cited_by_ui_url = metadata[\"cited_by_api_url\"].replace(\"api.openalex.org\", \"openalex.org\")\n",
    "        work_type = metadata.get(\"type\")\n",
    "        type_crossref = metadata.get(\"type_crossref\")\n",
    "        updated_date = metadata.get(\"updated_date\")\n",
    "\n",
    "        oa_data.append([\n",
    "            first_author_last_name, article_title, journal, publication_year,\n",
    "            publication_date, pmid, pmcid, oaid, pdf_url, doi_url,\n",
    "            cited_by_count, cited_by_ui_url, work_type, type_crossref, updated_date\n",
    "        ])\n",
    "\n",
    "    columns = [\n",
    "        'first_author_last_name', 'article_title', 'journal',\n",
    "        'publication_year', 'publication_date', 'pmid', 'pmcid', 'oaid',\n",
    "        'pdf_url', 'doi_url', 'cited_by_count', 'cited_by_ui_url', 'type',\n",
    "        'type_crossref', 'updated_date'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame with the specified columns\n",
    "    df_works = pd.DataFrame(oa_data, columns=columns, dtype=str)\n",
    "    df_works = df_works.drop_duplicates(subset=[\"pmid\"])\n",
    "    if exclude_errata:\n",
    "        df_works = df_works[df_works[\"type\"] != \"erratum\"]\n",
    "\n",
    "    # Parse the publication date as a datetime object with the format 'YYYY-MM-DD'\n",
    "    df_works[\"publication_date\"] = pd.to_datetime(df_works[\"publication_date\"], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Sort the DataFrame by publication date in descending order\n",
    "    df_works = df_works.sort_values(by=\"publication_date\", ascending=False)\n",
    "    df_works.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../utils/website_utils.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "def append_metadata(metadata_file_path: str, pmid_file_path: str, exclude_errata: bool = True, verbose: bool = True) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Append metadata for missing PMIDs to an existing metadata file.\n",
    "\n",
    "    Args:\n",
    "        metadata_file_path (str): Path to CSV file containing existing metadata.\n",
    "        pmid_file_path (str): Path to file containing list of PMIDs.\n",
    "        exclude_errata (bool): Whether to exclude errata from the metadata.\n",
    "        verbose (bool): Whether to show verbose messages during the process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a boolean indicating if any updates were made, and a message string with details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert metadata_file_path.endswith(\".csv\"), \"Invalid file format. Please provide a CSV file.\"\n",
    "    assert os.path.exists(metadata_file_path), \"Metadata file not found.\"\n",
    "    assert pmid_file_path.endswith(\".txt\"), \"Invalid file format. Please provide a TXT file.\"\n",
    "    assert os.path.exists(pmid_file_path), \"PMID file not found.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean.\"\n",
    "\n",
    "    # Read existing metadata\n",
    "    if verbose: print(\"Reading the existing metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(metadata_file_path, dtype=str)\n",
    "        metadata_bkp = deepcopy(metadata) # Make a deepcopy of the DataFrame to save a backup\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the metadata file: {e}\")\n",
    "        return False, f\"An error occurred while reading the metadata file: {e}\"\n",
    "\n",
    "    # Read PMIDs from file\n",
    "    if verbose: print(\"Reading the PMID file...\")\n",
    "    try:\n",
    "        with open(pmid_file_path, 'r') as f:\n",
    "            pmids = set(line.strip() for line in f)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the PMID file: {e}\")\n",
    "        return False, f\"An error occurred while reading the PMID file: {e}\"\n",
    "\n",
    "    # Find missing PMIDs\n",
    "    if verbose: print(\"Searching for new PMIDs not in the metadata...\")\n",
    "    existing_pmids: Set[str] = set(metadata[\"pmid\"])\n",
    "    new_pmids: Set[str] = pmids - existing_pmids\n",
    "    new_pmids_str = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "    if verbose: print(f\"Found {len(new_pmids)} new PMID(s): {new_pmids_str}.\")\n",
    "    if len(new_pmids) == 0:\n",
    "        return False, \"No new PMIDs found.\"\n",
    "    else:\n",
    "        try:\n",
    "            # Make API calls to get the missing PMIDs\n",
    "            select_fields: str = (\n",
    "                \"id,title,doi,primary_location,authorships,publication_year,\"\n",
    "                \"publication_date,ids,best_oa_location,cited_by_count,\"\n",
    "                \"cited_by_api_url,type,type_crossref,updated_date\"\n",
    "            )\n",
    "            new_articles, failed_calls = get_works(\n",
    "                ids=list(new_pmids),\n",
    "                email=os.environ.get(\"EMAIL\"),\n",
    "                select_fields=select_fields,\n",
    "                show_progress=verbose\n",
    "            )\n",
    "            if verbose: print(f\"API calls completed. Failed calls: {len(failed_calls)}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while fetching works data from the API: {e}\")\n",
    "            return False, f\"An error occurred while fetching works data from the API: {e}\"\n",
    "\n",
    "        # Parse the data for new articles\n",
    "        if verbose: print(\"Parsing the data for new articles...\")\n",
    "        try:\n",
    "            df_new_articles = parse_data(new_articles, exclude_errata=exclude_errata)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while parsing the data for new articles: {e}\")\n",
    "            return False, f\"An error occurred while parsing the data for new articles: {e}\"\n",
    "        \n",
    "        if df_new_articles.empty:\n",
    "            if exclude_errata:\n",
    "                if verbose: print(\"No new articles found (Errata excluded).\")\n",
    "                return False, \"No new articles found (Errata excluded).\"\n",
    "            else:\n",
    "                if verbose: print(\"No new articles found.\")\n",
    "                return False, \"No new articles found.\"\n",
    "        else:\n",
    "            # Append the new articles to the existing metadata\n",
    "            new_pmids = set(df_new_articles[\"pmid\"])\n",
    "            new_pmids = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "            if verbose: print(f\"Appending {len(df_new_articles)} new article(s) with PMID(s) {new_pmids} to the existing metadata...\")\n",
    "            try:\n",
    "                metadata = pd.concat([df_new_articles, metadata], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"An error occurred while appending the new articles to the existing metadata: {e}\")\n",
    "                return False, f\"An error occurred while appending the new articles to the existing metadata: {e}\"\n",
    "\n",
    "            # Save the updated metadata to a CSV file\n",
    "            if verbose: print(\"Saving the updated metadata to a CSV file...\")\n",
    "            metadata.to_csv(metadata_file_path, index=False)\n",
    "            if verbose: print(\"Saving a backup file to disk...\")\n",
    "            bkp_file_path = metadata_file_path.replace(\".csv\", f\"_bkp-{datetime.now().strftime('%Y%m%d-%Hh%Mm')}.csv\")\n",
    "            metadata_bkp.to_csv(bkp_file_path, index=False)\n",
    "            if verbose: print(\"Metadata updated successfully.\")\n",
    "\n",
    "            # Get the path to the log file from the metadata file path\n",
    "            log_file_path = os.path.join(os.path.dirname(metadata_file_path), \"update-log.json\")\n",
    "            \n",
    "            # Update the log file\n",
    "            try:\n",
    "                if verbose: print(\"Updating the log file...\")\n",
    "                with open(log_file_path, \"r\") as f:\n",
    "                    update_log = json.load(f)\n",
    "                current_date = datetime.now().strftime(\"%Y-%m-%d\")    \n",
    "                update_log[\"last_modified\"] = current_date # Expected format: {\"last_modified\": \"2024-08-06\"}\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump(update_log, f)\n",
    "                if verbose: print(f\"Log file updated successfully.\")\n",
    "            except Exception as e:\n",
    "                if verbose: print(f\"Error updating log file: {e}. Creating a new log file...\")\n",
    "                current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump({\"last_modified\": current_date}, f)\n",
    "                if verbose: print(f\"New log file created successfully.\")\n",
    "\n",
    "            return True, f\"Appended {len(df_new_articles)} article(s) and saved file to {metadata_file_path}. Backup saved as {bkp_file_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../utils/main.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from website_utils import update_citations, append_metadata\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Manage website metadata and citations.\")\n",
    "    \n",
    "    # Main operation group\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--update-citations\", action=\"store_true\", help=\"Update citation counts in the metadata file\")\n",
    "    group.add_argument(\"--append-metadata\", action=\"store_true\", help=\"Append metadata for missing PMIDs\")\n",
    "    group.add_argument(\"--update-and-append\", action=\"store_true\", help=\"Perform both update and append operations\")\n",
    "\n",
    "    # Common arguments\n",
    "    parser.add_argument(\"directory\", type=str, help=\"Directory containing the metadata, log, and PMID files\")\n",
    "    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode (no verbose output)\")\n",
    "    parser.add_argument(\"--include-errata\", action=\"store_true\", help=\"Include errata in the appended metadata\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Define file paths\n",
    "    metadata_file = os.path.join(args.directory, \"articles-metadata.csv\")\n",
    "    log_file = os.path.join(args.directory, \"update-log.json\")\n",
    "    pmid_file = os.path.join(args.directory, \"PMID-export.txt\")\n",
    "\n",
    "    # Validate file existence\n",
    "    if not os.path.exists(metadata_file):\n",
    "        parser.error(f\"Metadata file not found: {metadata_file}\")\n",
    "    if not os.path.exists(log_file):\n",
    "        parser.error(f\"Log file not found: {log_file}\")\n",
    "    if (args.append_metadata or args.update_and_append) and not os.path.exists(pmid_file):\n",
    "        parser.error(f\"PMID file not found: {pmid_file}\")\n",
    "\n",
    "    success_messages = []\n",
    "    error_messages = []\n",
    "\n",
    "    if args.update_citations or args.update_and_append:\n",
    "        success, message = update_citations(metadata_file, verbose=not args.quiet)\n",
    "        if success:\n",
    "            success_messages.append(f\"Update citations operation: {message}\")\n",
    "        else:\n",
    "            error_messages.append(f\"Update citations operation completed without saving new data: {message}\")\n",
    "\n",
    "    if args.append_metadata or args.update_and_append:\n",
    "        success, message = append_metadata(metadata_file, pmid_file, \n",
    "                                           exclude_errata=not args.include_errata, \n",
    "                                           verbose=not args.quiet)\n",
    "        if success:\n",
    "            success_messages.append(f\"Append metadata operation: {message}\")\n",
    "        else:\n",
    "            error_messages.append(f\"Append metadata operation completed without saving new data: {message}\")\n",
    "\n",
    "    # Print results\n",
    "    for message in success_messages:\n",
    "        print(message)\n",
    "    for message in error_messages:\n",
    "        print(message)\n",
    "\n",
    "    # Exit with error if any operation failed\n",
    "    if error_messages:\n",
    "        exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For execution from REPL / Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.expanduser(\"../utils\"))\n",
    "from website_utils import update_citations, parse_data, append_metadata\n",
    "from openalex_api_utils import get_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ~/GitHub/nicomarr.github.io/_data\n",
    "%ls ~/GitHub/nicomarr.github.io/_py/test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the data directory\n",
    "# data_dir = os.path.expanduser(\"~/GitHub/nicomarr.github.io/_data\")\n",
    "data_dir = os.path.expanduser(\"~/GitHub/nicomarr.github.io/_py/test_data\") # uncomment for local testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the data files\n",
    "pmid_list_path = os.path.join(data_dir, \"PMID-export.txt\")\n",
    "print(pmid_list_path)\n",
    "articles_metadata_path = os.path.join(data_dir, \"articles-metadata.csv\")\n",
    "print(articles_metadata_path)\n",
    "update_log_path = os.path.join(data_dir, \"update-log.json\")\n",
    "print(update_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_metadata(articles_metadata_path, pmid_list_path, exclude_errata=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_citations(articles_metadata_path, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For execution from command line\n",
    "\n",
    "***Make sure to activate the virtual environment before running the commands and that the required packages are installed!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "#### Update citations\n",
    "To update citations in the `test_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-citations ./_py/test_data\n",
    "```\n",
    "To update citations in the `_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-citations ./_data\n",
    "```\n",
    "Or in quiet mode:\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-citations ./_data --quiet\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "#### Append metadata \n",
    "To append metadata in the `test_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --append-metadata ./_py/test_data\n",
    "```\n",
    "\n",
    "To append metadata in the `_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --append-metadata ./_data\n",
    "```\n",
    "To include errata and run in quiet mode:\n",
    "```sh\n",
    "python ./_py/utils/main.py --append-metadata ./_data --quiet\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-note}\n",
    "#### Perform both update and append operations\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-and-append ./_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/GitHub/nicomarr.github.io/_py/utils/main.py --help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag4biomed-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
