{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code of helper functions to update publications page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read local data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ~/GitHub/nicomarr.github.io/_data\n",
    "# %ls ~/GitHub/nicomarr.github.io/_py/test_data # Uncomment for testing and development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.expanduser(\"~/GitHub/nicomarr.github.io/_data\")\n",
    "# data_dir = os.path.expanduser(\"~/GitHub/nicomarr.github.io/_py/test_data\") # Uncomment for local testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the data files\n",
    "pmid_list_path = os.path.join(data_dir, \"PMID-export.txt\")\n",
    "articles_metadata_path = os.path.join(data_dir, \"articles-metadata.csv\")\n",
    "update_log_path = os.path.join(data_dir, \"update-log.json\")\n",
    "print(f\"{pmid_list_path}\\n{articles_metadata_path}\\n{update_log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pmid_list_path) as f:\n",
    "    pmids = f.read().splitlines()\n",
    "print(f\"No. of PMIDs: {len(pmids)}\\n{pmids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_metadata = pd.read_csv(articles_metadata_path, dtype=str)\n",
    "df_articles_metadata[\"publication_date\"] = pd.to_datetime(df_articles_metadata[\"publication_date\"])\n",
    "df_articles_metadata = df_articles_metadata.sort_values(by=\"publication_date\", ascending=False)\n",
    "df_articles_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(update_log_path) as f:\n",
    "    update_log = json.load(f)\n",
    "update_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call OpenAlex API (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "id = \"34427831\" # pmid from the list; for testing\n",
    "base_url = \"https://api.openalex.org/works/\"\n",
    "params = {\n",
    "    \"mailto\": os.environ[\"EMAIL\"],\n",
    "    \"select\": \"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "}\n",
    "url = f\"{base_url}pmid:{id}\"\n",
    "\n",
    "verbose = True  # Define verbose or pass it as a parameter\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "    work = response.json()\n",
    "except requests.RequestException as e:\n",
    "    if verbose:\n",
    "        print(f\"An error occurred while making an API call with UID {id}: {e}\")\n",
    "    work = None\n",
    "except json.JSONDecodeError as e:\n",
    "    if verbose:\n",
    "        print(f\"Failed to decode JSON response for UID {id}: {e}\")\n",
    "        print(f\"Response content: {response.text}\")\n",
    "    work = None\n",
    "\n",
    "if work is None:\n",
    "    print(f\"Status code: {response.status_code}\")\n",
    "    print(\"Response headers:\")\n",
    "    pprint(dict(response.headers), indent=2)\n",
    "    print(\"Response content:\")\n",
    "    try:\n",
    "        pprint(response.json(), indent=2)\n",
    "    except json.JSONDecodeError:\n",
    "        print(response.text)\n",
    "else:\n",
    "    pprint(work, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `openalex_api_utils` to get metadata from the OpenAlex API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.expanduser(\"../utils\"))\n",
    "from openalex_api_utils import get_works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get metadata for a single entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works, failed_calls = get_works(\n",
    "    ids=[\"34427831\"], \n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print relevant metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(works[0][\"metadata\"][\"title\"])\n",
    "    print(works[0][\"metadata\"][\"authorships\"][0][\"author\"][\"display_name\"])\n",
    "    print(works[0][\"metadata\"][\"publication_date\"])\n",
    "    print(works[0][\"metadata\"][\"publication_year\"])\n",
    "    print(works[0][\"metadata\"][\"doi\"])\n",
    "    print(works[0][\"metadata\"][\"ids\"].get(\"pmid\").split(\"/\")[-1]) # PMID\n",
    "    print(works[0][\"metadata\"][\"ids\"].get(\"pmcid\").split(\"/\")[-1]) # PMCID\n",
    "    print(works[0][\"metadata\"][\"id\"]) # OpenAlex ID\n",
    "    print(works[0][\"metadata\"].get(\"best_oa_location\").get(\"pdf_url\")) # may be None\n",
    "    print(works[0][\"metadata\"][\"cited_by_count\"])\n",
    "    print(works[0][\"metadata\"][\"cited_by_api_url\"])\n",
    "    print(works[0][\"metadata\"].get(\"type\"))\n",
    "    print(works[0][\"metadata\"].get(\"type_crossref\"))\n",
    "    print(works[0][\"metadata\"][\"updated_date\"])\n",
    "except KeyError as e:\n",
    "    print(f\"KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get metadata from multiple entries and iterate over a list of works to extract relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works, failed_calls = get_works( \n",
    "    ids=pmids,\n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    show_progress=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(works), len(failed_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_data = []\n",
    "for work in works:\n",
    "    metadata = work[\"metadata\"]\n",
    "    first_author_last_name = metadata[\"authorships\"][0][\"author\"][\"display_name\"].split(\" \")[-1]\n",
    "    article_title = metadata[\"title\"]\n",
    "    journal = metadata[\"primary_location\"][\"source\"][\"display_name\"]\n",
    "    publication_year = str(metadata[\"publication_year\"])\n",
    "    publication_date = metadata[\"publication_date\"]\n",
    "    pmid = metadata[\"ids\"].get(\"pmid\", \"\").split(\"/\")[-1] # remove the url prefix\n",
    "    pmcid = metadata[\"ids\"].get(\"pmcid\")\n",
    "    if pmcid is not None:\n",
    "        pmcid = pmcid.split(\"/\")[-1] # remove the url prefix\n",
    "    else: \n",
    "        pmcid = \"NaN\"\n",
    "    oaid = metadata[\"id\"]\n",
    "    try:\n",
    "        pdf_url = metadata.get(\"best_oa_location\", {}).get(\"pdf_url\", \"not available\")\n",
    "    except AttributeError:\n",
    "        pdf_url = \"not available\"\n",
    "    if pdf_url is None:\n",
    "        pdf_url = \"not available\"\n",
    "    doi_url = metadata[\"doi\"]\n",
    "    cited_by_count = str(metadata[\"cited_by_count\"])\n",
    "    cited_by_ui_url = metadata[\"cited_by_api_url\"].replace(\"api.openalex.org\", \"openalex.org\")\n",
    "    type = metadata.get(\"type\")\n",
    "    type_crossref = metadata.get(\"type_crossref\")\n",
    "    updated_date = metadata.get(\"updated_date\")\n",
    "\n",
    "    # Append the extracted data to the list\n",
    "    oa_data.append([\n",
    "        first_author_last_name, article_title, journal, publication_year,\n",
    "        publication_date, pmid, pmcid, oaid, pdf_url,\n",
    "        doi_url, cited_by_count, cited_by_ui_url, type, type_crossref, updated_date\n",
    "    ])\n",
    "print(f\"{len(oa_data)} entries:\")\n",
    "print(json.dumps(oa_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the specified columns\n",
    "columns = [\n",
    "    'first_author_last_name', 'article_title', 'journal',\n",
    "    'publication_year', 'publication_date', 'pmid', 'pmcid', 'oaid',\n",
    "    'pdf_url', 'doi_url', 'cited_by_count', 'cited_by_ui_url', 'type',\n",
    "    'type_crossref', 'updated_date'\n",
    "]\n",
    "\n",
    "df_works = pd.DataFrame(oa_data, columns=columns)\n",
    "df_works = df_works.drop_duplicates(subset=[\"pmid\"])\n",
    "df_works = df_works[df_works[\"type\"] != \"erratum\"]\n",
    "df_works.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition to parse metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_data(works: List[Dict[str, Any]], exclude_errata = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the raw data from the OpenAlex API and create a DataFrame.\n",
    "\n",
    "    This function extracts relevant information from each work in the input list\n",
    "    and creates a DataFrame with specified columns. It also removes duplicates\n",
    "    based on PMID and filters out errata (if specified).\n",
    "\n",
    "    Args:\n",
    "        works (List[Dict[str, Any]]): A list of dictionaries, where each dictionary\n",
    "            contains metadata about a work.\n",
    "        exclude_errata (bool): Whether to exclude errata from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing extracted and processed information\n",
    "        from the works.\n",
    "\n",
    "    Example:\n",
    "        >>> df_works = create_works_dataframe(works)\n",
    "        >>> df_works.head()\n",
    "\n",
    "    Note:\n",
    "        The function extracts the following information for each work:\n",
    "        - First author's last name\n",
    "        - Article title\n",
    "        - Journal name\n",
    "        - Publication year and date\n",
    "        - PMID, PMCID, and OpenAlex ID\n",
    "        - PDF URL (if available)\n",
    "        - DOI URL\n",
    "        - Citation count and URL\n",
    "        - Work type and Crossref type\n",
    "        - Updated date (from the API)\n",
    "    \"\"\"\n",
    "    oa_data = []\n",
    "    for work in works:\n",
    "        metadata = work[\"metadata\"]\n",
    "        first_author_last_name = metadata[\"authorships\"][0][\"author\"][\"display_name\"].split(\" \")[-1]\n",
    "        article_title = metadata[\"title\"]\n",
    "        journal = metadata[\"primary_location\"][\"source\"][\"display_name\"]\n",
    "        publication_year = str(metadata[\"publication_year\"])\n",
    "        publication_date = metadata[\"publication_date\"]\n",
    "        pmid = metadata[\"ids\"].get(\"pmid\", \"\").split(\"/\")[-1] # remove the url prefix\n",
    "        pmcid = metadata[\"ids\"].get(\"pmcid\")\n",
    "        if pmcid is not None:\n",
    "            pmcid = pmcid.split(\"/\")[-1] # remove the url prefix\n",
    "        else: \n",
    "            pmcid = \"NaN\"\n",
    "        oaid = metadata[\"id\"]\n",
    "        try:\n",
    "            pdf_url = metadata.get(\"best_oa_location\", {}).get(\"pdf_url\", \"not available\")\n",
    "        except AttributeError:\n",
    "            pdf_url = \"not available\"\n",
    "        if pdf_url is None:\n",
    "            pdf_url = \"not available\"\n",
    "        doi_url = metadata[\"doi\"]\n",
    "        cited_by_count = str(metadata[\"cited_by_count\"])\n",
    "        cited_by_ui_url = metadata[\"cited_by_api_url\"].replace(\"api.openalex.org\", \"openalex.org\")\n",
    "        work_type = metadata.get(\"type\")\n",
    "        type_crossref = metadata.get(\"type_crossref\")\n",
    "        updated_date = metadata.get(\"updated_date\")\n",
    "\n",
    "        oa_data.append([\n",
    "            first_author_last_name, article_title, journal, publication_year,\n",
    "            publication_date, pmid, pmcid, oaid, pdf_url, doi_url,\n",
    "            cited_by_count, cited_by_ui_url, work_type, type_crossref, updated_date\n",
    "        ])\n",
    "\n",
    "    columns = [\n",
    "        'first_author_last_name', 'article_title', 'journal',\n",
    "        'publication_year', 'publication_date', 'pmid', 'pmcid', 'oaid',\n",
    "        'pdf_url', 'doi_url', 'cited_by_count', 'cited_by_ui_url', 'type',\n",
    "        'type_crossref', 'updated_date'\n",
    "    ]\n",
    "\n",
    "    df_works = pd.DataFrame(oa_data, columns=columns, dtype=str)\n",
    "    df_works = df_works.drop_duplicates(subset=[\"pmid\"])\n",
    "    if exclude_errata:\n",
    "        df_works = df_works[df_works[\"type\"] != \"erratum\"]\n",
    "\n",
    "    df_works[\"publication_date\"] = pd.to_datetime(df_works[\"publication_date\"], format='%Y-%m-%d', errors='coerce')\n",
    "    df_works = df_works.sort_values(by=\"publication_date\", ascending=False)\n",
    "    df_works.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_works = parse_data(works)\n",
    "df_works.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare local data with external data and find mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the shapes of the DataFrames\n",
    "print(\"df_works shape:\", df_works.shape)\n",
    "print(\"df_articles_metadata shape:\", df_articles_metadata.shape)\n",
    "assert df_works.shape == df_articles_metadata.shape, \"DataFrames have different shapes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the column names:\n",
    "print(\"df_works columns:\", df_works.columns.tolist())\n",
    "print(\"articles_metadata columns:\", df_articles_metadata.columns.tolist())\n",
    "assert df_works.columns.tolist() == df_articles_metadata.columns.tolist(), \"Column names are different.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common columns:\n",
    "common_columns = set(df_works.columns) & set(df_articles_metadata.columns)\n",
    "print(\"Common columns:\", common_columns)\n",
    "print(\"No. of common columns:\", len(common_columns))\n",
    "\n",
    "# Check for columns only in one of the DataFrames:\n",
    "works_columns = set(df_works.columns)\n",
    "articles_metadata_columns = set(df_articles_metadata.columns)\n",
    "print(\"Columns only in df_works:\", works_columns - articles_metadata_columns)\n",
    "print(\"Columns only in articles_metadata:\", articles_metadata_columns - works_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the datatypes:\n",
    "common_columns = list(common_columns)\n",
    "df_works_dtypes = df_works[common_columns].dtypes\n",
    "articles_metadata_dtypes = df_articles_metadata[common_columns].dtypes\n",
    "dtypes_comparison = df_works_dtypes == articles_metadata_dtypes\n",
    "\n",
    "print(\"Columns with mismatched datatypes:\")\n",
    "for column in common_columns:\n",
    "    if df_works_dtypes[column] == articles_metadata_dtypes[column]:\n",
    "        continue\n",
    "    print(f\"{column}:\")\n",
    "    print(f\"  df_works: {df_works_dtypes[column]}\")\n",
    "    print(f\"  articles_metadata: {articles_metadata_dtypes[column]}\")\n",
    "    print(f\"  Match: {dtypes_comparison[column]}\")\n",
    "    print()\n",
    "\n",
    "# leave pmid dtypes as int for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the \"pmid\" values:\n",
    "df_works_pmids = set(df_works[\"pmid\"])\n",
    "articles_metadata_pmids = set(df_articles_metadata[\"pmid\"].astype(str))\n",
    "common_pmids = df_works_pmids & articles_metadata_pmids\n",
    "\n",
    "print(\"Number of common PMIDs:\", len(common_pmids))\n",
    "print(\"PMIDs only in df_works:\", len(df_works_pmids - articles_metadata_pmids))\n",
    "print(\"PMIDs only in articles_metadata:\", len(articles_metadata_pmids - df_works_pmids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the values for article_title in both DataFrames\n",
    "series1 = df_works[\"article_title\"].reset_index(drop=True)\n",
    "series2 = df_articles_metadata[\"article_title\"].reset_index(drop=True)\n",
    "title_mismatch = series1.compare(series2)\n",
    "title_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the values for pdf_url in both DataFrames\n",
    "series1 = df_works[\"pdf_url\"].reset_index(drop=True)\n",
    "series2 = df_articles_metadata[\"pdf_url\"].reset_index(drop=True)\n",
    "pdf_url_mismatch = series1.compare(series2)\n",
    "pdf_url_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the values for the doi_url in both DataFrames\n",
    "series1 = df_works[\"doi_url\"].reset_index(drop=True)\n",
    "series2 = df_articles_metadata[\"doi_url\"].reset_index(drop=True)\n",
    "pdf_url_mismatch = series1.compare(series2)\n",
    "pdf_url_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all columns\n",
    "result = df_works.reset_index(drop=True).compare(df_articles_metadata.reset_index(drop=True))\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definition to add metadata of new articles to the local/existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids.append(\"39198650\") # Add a new PMID, for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find IDs in the PMID list missing from the local metadata\n",
    "new_pmids = set(pmids) - set(df_articles_metadata[\"pmid\"].astype(str))\n",
    "new_pmids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make API calls to get the missing PMIDs\n",
    "exclude_errata=True\n",
    "new_works, failed_calls = get_works(\n",
    "    ids=list(new_pmids),\n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,title,doi,primary_location,authorships,publication_year,publication_date,ids,best_oa_location,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "df_new_works = parse_data(new_works, exclude_errata=exclude_errata)\n",
    "if len(df_new_works) == 0:\n",
    "    print(\"No new articles found.\")\n",
    "    print(f\"Failed calls: {len(failed_calls)}\")\n",
    "    print(f\"Errata excluded: {exclude_errata}\")\n",
    "    print(f\"No. of errata in new works: {len(df_new_works[df_new_works['type'] == 'erratum'])}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n{len(df_new_works)} new article(s) found.\\n\")\n",
    "    print(df_new_works[[\"article_title\"]])\n",
    "# assert len(df_new_works) == 1, \"Number of new articles is not match the expected number.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_new_works) == 0:\n",
    "    print(\"No new articles found.\")\n",
    "else:\n",
    "    df_updated_works = pd.concat([df_works, df_new_works], ignore_index=False)\n",
    "    df_updated_works = df_updated_works.sort_values(by=\"publication_date\", ascending=False)\n",
    "    assert df_new_works.at[0, 'article_title'] in df_updated_works[\"article_title\"].values\n",
    "    df_updated_works.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from typing import Tuple, Set, List, Dict, Any\n",
    "\n",
    "def append_metadata(metadata_file_path: str, pmid_file_path: str, exclude_errata: bool = True, verbose: bool = True) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Append metadata for missing PMIDs to an existing metadata file.\n",
    "\n",
    "    Args:\n",
    "        metadata_file_path (str): Path to CSV file containing existing metadata.\n",
    "        pmid_file_path (str): Path to file containing list of PMIDs.\n",
    "        verbose (bool): Whether to show verbose messages during the process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a boolean indicating if any updates were made, and a message string with details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert metadata_file_path.endswith(\".csv\"), \"Invalid file format. Please provide a CSV file.\"\n",
    "    assert os.path.exists(metadata_file_path), \"Metadata file not found.\"\n",
    "    assert pmid_file_path.endswith(\".txt\"), \"Invalid file format. Please provide a TXT file.\"\n",
    "    assert os.path.exists(pmid_file_path), \"PMID file not found.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean.\"\n",
    "\n",
    "    # Read existing metadata\n",
    "    if verbose: print(\"Reading the existing metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(metadata_file_path, dtype=str)\n",
    "        metadata_bkp = deepcopy(metadata) # Make a deepcopy of the DataFrame to save a backup\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the metadata file: {e}\")\n",
    "        return False, f\"An error occurred while reading the metadata file: {e}\"\n",
    "\n",
    "    # Read PMIDs from file\n",
    "    if verbose: print(\"Reading the PMID file...\")\n",
    "    try:\n",
    "        with open(pmid_file_path, 'r') as f:\n",
    "            pmids = set(line.strip() for line in f)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the PMID file: {e}\")\n",
    "        return False, f\"An error occurred while reading the PMID file: {e}\"\n",
    "\n",
    "    # Find missing PMIDs\n",
    "    if verbose: print(\"Searching for new PMIDs not in the metadata...\")\n",
    "    existing_pmids: Set[str] = set(metadata[\"pmid\"])\n",
    "    new_pmids: Set[str] = pmids - existing_pmids\n",
    "    new_pmids_str = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "    if verbose: print(f\"Found {len(new_pmids)} new PMID(s): {new_pmids_str}.\")\n",
    "    if len(new_pmids) == 0:\n",
    "        return False, \"No new PMIDs found.\"\n",
    "    else:\n",
    "        try:\n",
    "            # Make API calls to get the missing PMIDs\n",
    "            exclude_errata: bool = True\n",
    "            select_fields: str = (\n",
    "                \"id,title,doi,primary_location,authorships,publication_year,\"\n",
    "                \"publication_date,ids,best_oa_location,cited_by_count,\"\n",
    "                \"cited_by_api_url,type,type_crossref,updated_date\"\n",
    "            )\n",
    "            new_articles, failed_calls = get_works(\n",
    "                ids=list(new_pmids),\n",
    "                email=os.environ.get(\"EMAIL\"),\n",
    "                select_fields=select_fields,\n",
    "                show_progress=verbose\n",
    "            )\n",
    "            if verbose: print(f\"API calls completed. Failed calls: {len(failed_calls)}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while fetching works data from the API: {e}\")\n",
    "            return False, f\"An error occurred while fetching works data from the API: {e}\"\n",
    "\n",
    "        # Parse the data for new articles\n",
    "        if verbose: print(\"Parsing the data for new articles...\")\n",
    "        try:\n",
    "            df_new_articles = parse_data(new_articles, exclude_errata=True)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while parsing the data for new articles: {e}\")\n",
    "            return False, f\"An error occurred while parsing the data for new articles: {e}\"\n",
    "        \n",
    "        if df_new_articles.empty:\n",
    "            if exclude_errata:\n",
    "                if verbose: print(\"No new articles found (Errata excluded).\")\n",
    "                return False, \"No new articles found (Errata excluded).\"\n",
    "            else:\n",
    "                if verbose: print(\"No new articles found.\")\n",
    "                return False, \"No new articles found.\"\n",
    "        else:\n",
    "            # Append the new articles to the existing metadata\n",
    "            new_pmids = set(df_new_articles[\"pmid\"])\n",
    "            new_pmids = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "            if verbose: print(f\"Appending {len(df_new_articles)} new article(s) with PMID(s) {new_pmids} to the existing metadata...\")\n",
    "            try:\n",
    "                metadata = pd.concat([metadata, df_new_articles], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"An error occurred while appending the new articles to the existing metadata: {e}\")\n",
    "                return False, f\"An error occurred while appending the new articles to the existing metadata: {e}\"\n",
    "\n",
    "            # Save the updated metadata to a CSV file\n",
    "            if verbose: print(\"Saving the updated metadata to a CSV file...\")\n",
    "            metadata.to_csv(metadata_file_path, index=False)\n",
    "            if verbose: print(\"Saving a backup file to disk...\")\n",
    "            bkp_file_path = metadata_file_path.replace(\".csv\", f\"_bkp-{datetime.now().strftime('%Y%m%d-%Hh%Mm')}.csv\")\n",
    "            metadata_bkp.to_csv(bkp_file_path, index=False)\n",
    "            if verbose: print(\"Metadata updated successfully.\")\n",
    "\n",
    "            # Update the log file\n",
    "            log_file_path = os.path.join(os.path.dirname(metadata_file_path), \"update-log.json\")\n",
    "            try:\n",
    "                # get the path to the log file from the metadata file path\n",
    "                if verbose: print(\"Updating the log file...\")\n",
    "                with open(log_file_path, \"r\") as f:\n",
    "                    update_log = json.load(f)\n",
    "                # format {\"last_modified\": \"2024-08-06\"}\n",
    "                update_log[\"last_modified\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump(update_log, f)\n",
    "                if verbose: print(f\"Log file updated successfully.\")\n",
    "            except Exception as e: # Note: Additional error handling has been added in the final version below\n",
    "                if verbose: print(f\"No log file found. Error: {e}. Creating a new log file...\")\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump({\"last_modified\": datetime.now().strftime(\"%Y-%m-%d\")}, f)\n",
    "                if verbose: print(f\"Log file created successfully.\")\n",
    "\n",
    "            return True, f\"Appended {len(df_new_articles)} article(s) and saved file to {metadata_file_path}. Backup saved as {bkp_file_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to update citation counts of all articles in the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset a row in df_articles_metadata based on a condition\n",
    "# df_articles_metadata[df_articles_metadata[\"doi_url\"] == \"https://doi.org/10.1038/s41586-024-07745-x\"]\n",
    "df_articles_metadata[df_articles_metadata[\"doi_url\"] == \"https://doi.org/10.12688/f1000research.18048.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a specific article by PMID; for debugging\n",
    "pmid = \"31559014\"\n",
    "df_articles_metadata[df_articles_metadata[\"pmid\"] == pmid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get up-to-date metadata for articles in the local metadata file; only cited_by_count and updated_date will be modified\n",
    "works, failed_calls = get_works( \n",
    "    ids=df_articles_metadata[\"doi_url\"].astype(str).tolist(),\n",
    "    email=os.environ.get(\"EMAIL\"),\n",
    "    select_fields=\"id,ids,doi,title,cited_by_count,updated_date\",\n",
    "    show_progress=True\n",
    ")\n",
    "assert len(works) == len(df_articles_metadata), \"Number of works does not match the number of articles in the metadata.\"\n",
    "if len(failed_calls) > 0:\n",
    "    print(f\"Failed calls: {failed_calls}\")\n",
    "else:\n",
    "    print(\"All API calls successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(works), len(failed_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the citation counts\n",
    "for work in works:\n",
    "    print(f\"{work[\"metadata\"][\"title\"][:50]}... {work[\"metadata\"][\"doi\"].replace(\"https://doi.org/\",\"\")}, Citation count: {work[\"metadata\"][\"cited_by_count\"]}, {work[\"metadata\"][\"id\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop for debugging\n",
    "id = \"https://openalex.org/W2914632197\"\n",
    "work = next((work for work in works if work[\"metadata\"][\"id\"] == id), None)\n",
    "if work is not None:\n",
    "    print(f\"{work['metadata']['title'][:50]}... {work['metadata']['doi'].replace('https://doi.org/','')}, Citation count: {work['metadata']['cited_by_count']}, {work['metadata']['id']}\")\n",
    "else:\n",
    "    print(f\"Work with ID {id} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata file\n",
    "df_articles_metadata = pd.read_csv(articles_metadata_path, dtype=str)\n",
    "df_articles_metadata[\"publication_date\"] = pd.to_datetime(df_articles_metadata[\"publication_date\"])\n",
    "df_articles_metadata = df_articles_metadata.sort_values(by=\"publication_date\", ascending=False)\n",
    "\n",
    "# Iterate over the rows in df_articles_metadata and update the cited_by_count and updated_date\n",
    "counter = 0\n",
    "\n",
    "# make a deepcopy of the DataFrame to save a backup\n",
    "from copy import deepcopy\n",
    "df_articles_metadata_bkp = deepcopy(df_articles_metadata)\n",
    "\n",
    "for index, row in df_articles_metadata.iterrows():\n",
    "    id = row[\"oaid\"]\n",
    "    title = row[\"article_title\"]\n",
    "    current_cited_by_count = row[\"cited_by_count\"]\n",
    "    work = next((work for work in works if work[\"metadata\"][\"id\"] == id), None)\n",
    "    try:\n",
    "        new_cited_by_count = work[\"metadata\"][\"cited_by_count\"] # <-- raises TypeError: 'NoneType' object is not subscriptable for one of the PMIDs, https://api.openalex.org/works/W2914632197\n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError: {e}\") # For debugging\n",
    "        print(f\"Work: {work}\") # For debugging\n",
    "        print(f\"row: {row[\"type\"]}\") # For debugging\n",
    "        continue\n",
    "    if new_cited_by_count > int(current_cited_by_count):\n",
    "        try:\n",
    "            print(f\"Updating the cited_by_count for {id.split('/')[-1]}: {title[:50]} from {current_cited_by_count} to {new_cited_by_count}\")\n",
    "            df_articles_metadata.at[index, \"cited_by_count\"] = new_cited_by_count\n",
    "            df_articles_metadata.at[index, \"updated_date\"] = work[\"metadata\"][\"updated_date\"]\n",
    "            counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to update the cited_by_count for PMID: {pmid}\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(f\"Citation count for ID: {id.split('/')[-1]} is up-to-date. Citattion count: {current_cited_by_count}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "print(f\"Updated values for {counter} articles.\")\n",
    "if counter > 0:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import Tuple, Set, List, Dict, Any\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"../utils\"))\n",
    "from openalex_api_utils import get_works\n",
    "\n",
    "def update_citations(\n",
    "    file_path: str,\n",
    "    save_metadata_to_disk: bool = True,\n",
    "    save_backup: bool = True,\n",
    "    save_log_file: bool = True, \n",
    "    verbose: bool = True\n",
    ") -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Update citation counts in articles metadata file using OpenAlex API data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the articles metadata CSV file.\n",
    "        save_metadata_to_disk (bool): Whether to save the updated metadata to disk. Default is True. Set to False for testing on actual metadata.\n",
    "        save_backup (bool): Whether to save a backup of the original metadata Default is True. Set to False for testing on actual metadata.\n",
    "        save_log_file (bool): Whether to update the log file. Default is True. Set to False for testing on actual metadata.\n",
    "        verbose (bool): Whether to show detailed progress messages. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, str]: (success status, detailed message)\n",
    "    \"\"\"\n",
    "    # Basic input validation\n",
    "    file_path = os.path.expanduser(file_path) # Expand relative paths to absolute paths\n",
    "    if not os.path.exists(file_path):\n",
    "        return False, f\"File not found: {file_path}\"\n",
    "    if not file_path.endswith('.csv'):\n",
    "        return False, \"Invalid file format. Must be CSV.\"\n",
    "    assert isinstance(save_metadata_to_disk, bool), \"save_to_disk must be a boolean.\"\n",
    "    assert isinstance(save_backup, bool), \"save_backup must be a boolean.\"\n",
    "    assert isinstance(save_log_file, bool), \"save_log_file must be a boolean.\"\n",
    "    assert isinstance(verbose, bool), \"verbose must be a boolean.\"\n",
    "\n",
    "    # Read metadata file\n",
    "    if verbose:\n",
    "        print(\"Reading metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(file_path, dtype=str)\n",
    "        metadata[\"publication_date\"] = pd.to_datetime(metadata[\"publication_date\"])\n",
    "        metadata = metadata.sort_values(by=\"publication_date\", ascending=False)\n",
    "        \n",
    "        if metadata.empty:\n",
    "            return False, \"Empty metadata file\"\n",
    "        \n",
    "        required_cols = ['oaid', 'cited_by_count', 'updated_date', 'doi_url']\n",
    "        if not all(col in metadata.columns for col in required_cols):\n",
    "            return False, f\"Missing required columns: {set(required_cols) - set(metadata.columns)}\"\n",
    "            \n",
    "        metadata_backup = deepcopy(metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading metadata file: {str(e)}\"\n",
    "\n",
    "    # Fetch works data from OpenAlex API\n",
    "    if verbose:\n",
    "        print(\"Calling OpenAlex API ...\")\n",
    "    try:\n",
    "        valid_ids = []\n",
    "        for _, row in metadata.iterrows():\n",
    "            oaid = str(row['oaid'])\n",
    "            if pd.notna(oaid):\n",
    "                oaid_clean = oaid.split('/')[-1] if '/' in oaid else oaid\n",
    "                valid_ids.append(oaid_clean)\n",
    "\n",
    "        if not valid_ids:\n",
    "            return False, \"No valid OpenAlex IDs found\"\n",
    "\n",
    "        works, failed_calls = get_works(\n",
    "            ids=valid_ids,\n",
    "            email=os.getenv(\"EMAIL\"),\n",
    "            select_fields=\"id,doi,cited_by_count,updated_date\",\n",
    "            show_progress=verbose\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return False, f\"API error: {str(e)}\"\n",
    "\n",
    "    # Update citation counts\n",
    "    updated_count = 0\n",
    "    errors = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Updating citation counts...\")\n",
    "    for idx, row in metadata.iterrows():\n",
    "        try:\n",
    "            oaid = str(row[\"oaid\"])\n",
    "            doi = row[\"doi_url\"]\n",
    "            current_citations = int(row[\"cited_by_count\"]) if pd.notna(row[\"cited_by_count\"]) else 0\n",
    "            \n",
    "            work = next((w for w in works if w[\"metadata\"][\"id\"] == oaid), None)\n",
    "            \n",
    "            if not work:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                new_citations = work[\"metadata\"][\"cited_by_count\"]\n",
    "            except TypeError as e:\n",
    "                if verbose:\n",
    "                    print(f\"TypeError: {e}\")\n",
    "                    print(f\"Work: {work}\")\n",
    "                    print(f\"Row type: {row['type']}\")\n",
    "                continue\n",
    "\n",
    "            if new_citations > current_citations:\n",
    "                if verbose:\n",
    "                    print(f\"Updating citations for OAID: {oaid} / DOI: {doi} from {current_citations} to {new_citations}\")\n",
    "                metadata.at[idx, 'cited_by_count'] = str(new_citations)\n",
    "                metadata.at[idx, 'updated_date'] = work[\"metadata\"][\"updated_date\"]\n",
    "                updated_count += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Citation count for OAID: {oaid} / DOI: {doi} is up-to-date. Citation count: {current_citations}. Skipping...\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error processing {oaid}: {str(e)}\")\n",
    "            if verbose:\n",
    "                print(f\"Failed to update the cited_by_count for ID: {oaid}\")\n",
    "                print(e)\n",
    "            continue\n",
    "\n",
    "    # Save updates if any were made\n",
    "    if updated_count > 0:\n",
    "        if save_metadata_to_disk:\n",
    "            if save_backup:\n",
    "                try:\n",
    "                    if verbose:\n",
    "                        print(\"Saving a backup of the original metadata file...\")\n",
    "                    backup_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "                    backup_path = file_path.replace(\".csv\", f\"_bkp-{backup_timestamp}.csv\")\n",
    "                    metadata_backup.to_csv(backup_path, index=False)\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error saving backup: {str(e)}\"\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Saving updated metadata to disk...\")\n",
    "            try:\n",
    "                metadata.to_csv(file_path, index=False)\n",
    "            except Exception as e:\n",
    "                return False, f\"Error saving updated metadata to disk: {str(e)}\"\n",
    "            \n",
    "            if save_log_file:\n",
    "                if verbose:\n",
    "                    print(\"Updating the log file...\")\n",
    "                try:\n",
    "                    log_data = {\n",
    "                        \"last_modified\": datetime.now().strftime('%Y-%m-%d'),\n",
    "                        \"status_message\": f\"Successfully updated citation counts for {updated_count} articles\",\n",
    "                    }\n",
    "                    with open(os.path.join(os.path.dirname(file_path), \"update-log.json\"), 'w') as f:\n",
    "                        json.dump(log_data, f, indent=2)\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error updating log file: {str(e)}\"\n",
    "        \n",
    "            return True, f\"Successfully updated citation counts for {updated_count} articles and saved metadata to disk.\"\n",
    "        else: \n",
    "            return True, f\"Successfully updated citation counts for {updated_count} articles. No changes saved to disk.\"\n",
    "    else:\n",
    "        return True, \"No updates made. Citation counts are up-to-date.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test cases for update_citations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ~/GitHub/nicomarr.github.io/_data/articles-metadata_bkp-20241018-16h30m.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function without saving the metadata to disk\n",
    "file_path = \"~/GitHub/nicomarr.github.io/_data/articles-metadata_bkp-20241018-16h30m.csv\"\n",
    "update_citations(\n",
    "    file_path, \n",
    "    save_metadata_to_disk=False, \n",
    "    save_backup=False, \n",
    "    save_log_file=False,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ~/GitHub/nicomarr.github.io/_py/test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function on the actual metadata file in the test_data directory, update the metadata file and log file\n",
    "file_path = \"~/GitHub/nicomarr.github.io/_py/test_data/articles-metadata.csv\"\n",
    "update_citations(\n",
    "    file_path, \n",
    "    save_metadata_to_disk=True, \n",
    "    save_backup=True, \n",
    "    save_log_file=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the backup file in test directory to remove the timestamp, overwrite the existing metadata file for subsequent tests\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "test_dir_path = Path(\"~/GitHub/nicomarr.github.io/_py/test_data/\").expanduser()\n",
    "backup_files = sorted(test_dir_path.glob(\"articles-metadata_bkp-*.csv\"), key=lambda p: p.stat().st_mtime)\n",
    "try:\n",
    "    last_backup_file = backup_files[-1]\n",
    "    new_name = re.sub(r'_bkp-\\d{8}-\\d{6}', '', last_backup_file.stem) + '.csv'\n",
    "    last_backup_file.rename(last_backup_file.parent / new_name)\n",
    "    print(f\"Renamed {last_backup_file.name} to {new_name}\")\n",
    "except IndexError:\n",
    "    print(\"No backup files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive test cases for update_citations function\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, UTC\n",
    "import json\n",
    "\n",
    "def test_update_citations():\n",
    "    \"\"\"Run comprehensive test cases for update_citations function\"\"\"\n",
    "    \n",
    "    print(\"Running update_citations tests...\\n\")\n",
    "    results = []\n",
    "    \n",
    "    def run_test(name, test_function):\n",
    "        \"\"\"Helper to run a test and format results\"\"\"\n",
    "        try:\n",
    "            success, message = test_function()\n",
    "            status = \"PASSED\" if success else \"FAILED\"\n",
    "            print(f\"{name}: {status}\")\n",
    "            print(f\"Message: {message}\\n\")\n",
    "            return {'name': name, 'status': status, 'message': message}\n",
    "        except Exception as e:\n",
    "            print(f\"{name}: ERROR\")\n",
    "            print(f\"Error: {str(e)}\\n\")\n",
    "            return {'name': name, 'status': 'ERROR', 'message': str(e)}\n",
    "\n",
    "    def test_valid_update():\n",
    "        \"\"\"Test successful citation update with actual data\"\"\"\n",
    "        test_file = 'test_metadata.csv'\n",
    "        \n",
    "        # Create test data matching actual metadata structure\n",
    "        test_data = pd.DataFrame({\n",
    "            'oaid': ['https://openalex.org/W3194117818'],\n",
    "            'cited_by_count': ['5'],  # Citation count to be updated\n",
    "            'updated_date': ['2024-07-13T16:04:21.884242'],\n",
    "            'doi_url': ['https://doi.org/10.1007/s10875-021-01115-2'],\n",
    "            'first_author_last_name': ['Guennoun'],\n",
    "            'article_title': ['Test Article'],\n",
    "            'journal': ['Test Journal'],\n",
    "            'publication_year': ['2021'],\n",
    "            'publication_date': ['2021-08-24'],\n",
    "            'type': ['article'],\n",
    "            'type_crossref': ['journal-article']\n",
    "        })\n",
    "        \n",
    "        test_data.to_csv(test_file, index=False)\n",
    "\n",
    "        # Create a log file for testing\n",
    "        log_data = {\n",
    "            \"last_modified\": \"2024-07-13\",\n",
    "            \"status_message\": \"Initial metadata file created\"\n",
    "        }\n",
    "        with open(\"update-log.json\", 'w') as f:\n",
    "            json.dump(log_data, f, indent=2)\n",
    "        \n",
    "        try:\n",
    "            # Run update\n",
    "            success, message = update_citations(\n",
    "                test_file, \n",
    "                save_metadata_to_disk=True,\n",
    "                save_backup=True,\n",
    "                save_log_file=False,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            # Read updated file\n",
    "            if os.path.exists(test_file):\n",
    "                updated_data = pd.read_csv(test_file)\n",
    "                print(\"\\nDebug Information:\")\n",
    "                print(f\"Update success: {success}\")\n",
    "                print(f\"Update message: {message}\")\n",
    "                \n",
    "                # Verify the update\n",
    "                new_count = updated_data['cited_by_count'].iloc[0]\n",
    "                \n",
    "                # Convert new count to int for comparison\n",
    "                if int(new_count) < 6:\n",
    "                    return False, f\"Expected citation count >5, but got {new_count}\"\n",
    "                \n",
    "                # Return success with specific message about the update\n",
    "                return True, f\"Citation count successfully updated from 5 to {new_count}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return False, f\"Test failed with error: {str(e)}\"\n",
    "            \n",
    "        finally:\n",
    "            # Clean up\n",
    "            if os.path.exists(test_file):\n",
    "                os.remove(test_file)\n",
    "            for f in os.listdir():\n",
    "                if f.startswith(\"test_metadata_bkp-\"):\n",
    "                    os.remove(f)\n",
    "            if os.path.exists(\"update-log.json\"):\n",
    "                os.remove(\"update-log.json\")\n",
    "\n",
    "    def test_invalid_format():\n",
    "        \"\"\"Test handling of invalid file format\"\"\"\n",
    "        test_file = 'test.txt'\n",
    "        with open(test_file, 'w') as f:\n",
    "            f.write(\"test\")\n",
    "            \n",
    "        try:\n",
    "            success, message = update_citations(test_file)\n",
    "            \n",
    "            if success:\n",
    "                return False, \"Should fail for invalid file format\"\n",
    "            if \"Invalid file format\" not in message:\n",
    "                return False, \"Wrong error message for invalid format\"\n",
    "                \n",
    "            return True, \"Correctly handled invalid format\"\n",
    "            \n",
    "        finally:\n",
    "            if os.path.exists(test_file):\n",
    "                os.remove(test_file)\n",
    "\n",
    "    def test_missing_file():\n",
    "        \"\"\"Test handling of non-existent file\"\"\"\n",
    "        success, message = update_citations(\"nonexistent.csv\")\n",
    "        \n",
    "        if success:\n",
    "            return False, \"Should fail for missing file\"\n",
    "        if \"File not found\" not in message:\n",
    "            return False, \"Wrong error message for missing file\"\n",
    "            \n",
    "        return True, \"Correctly handled missing file\"\n",
    "\n",
    "    def test_empty_file():\n",
    "        \"\"\"Test handling of empty CSV file\"\"\"\n",
    "        test_file = \"empty.csv\"\n",
    "        \n",
    "        # Create empty file with correct columns\n",
    "        pd.DataFrame(columns=[\n",
    "            'oaid', 'cited_by_count', 'updated_date', 'doi_url',\n",
    "            'first_author_last_name', 'article_title', 'journal',\n",
    "            'publication_year', 'publication_date', 'type', 'type_crossref'\n",
    "        ]).to_csv(test_file, index=False)\n",
    "        \n",
    "        try:\n",
    "            success, message = update_citations(test_file)\n",
    "            \n",
    "            if success:\n",
    "                return False, \"Should fail for empty file\"\n",
    "            if \"Empty metadata file\" not in message:\n",
    "                return False, \"Wrong error message for empty file\"\n",
    "                \n",
    "            return True, \"Correctly handled empty file\"\n",
    "            \n",
    "        finally:\n",
    "            if os.path.exists(test_file):\n",
    "                os.remove(test_file)\n",
    "\n",
    "    # Run all tests\n",
    "    tests = [\n",
    "        (\"Valid Update\", test_valid_update),\n",
    "        (\"Invalid Format\", test_invalid_format),\n",
    "        (\"Missing File\", test_missing_file),\n",
    "        (\"Empty File\", test_empty_file)\n",
    "    ]\n",
    "\n",
    "    for test_name, test_func in tests:\n",
    "        results.append(run_test(test_name, test_func))\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nTest Summary:\")\n",
    "    passed = sum(1 for r in results if r['status'] == 'PASSED')\n",
    "    failed = sum(1 for r in results if r['status'] == 'FAILED')\n",
    "    errors = sum(1 for r in results if r['status'] == 'ERROR')\n",
    "    \n",
    "    print(f\"Passed: {passed}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "    print(f\"Errors: {errors}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run tests\n",
    "if __name__ == \"__main__\":\n",
    "    test_results = test_update_citations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate module to execute the functions from the command line interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../utils/__init__.py\n",
    "__version__ = \"0.0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../utils/website_utils.py\n",
    "import os\n",
    "import json\n",
    "from typing import Tuple, Set, List, Dict, Any\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from openalex_api_utils import get_works\n",
    "\n",
    "def update_citations(\n",
    "    file_path: str,\n",
    "    save_metadata_to_disk: bool = True,\n",
    "    save_backup: bool = True,\n",
    "    save_log_file: bool = True, \n",
    "    verbose: bool = True\n",
    ") -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Update citation counts in articles metadata file using OpenAlex API data.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the articles metadata CSV file.\n",
    "        save_metadata_to_disk (bool): Whether to save the updated metadata to disk. Default is True. Set to False for testing on actual metadata.\n",
    "        save_backup (bool): Whether to save a backup of the original metadata Default is True. Set to False for testing on actual metadata.\n",
    "        save_log_file (bool): Whether to update the log file. Default is True. Set to False for testing on actual metadata.\n",
    "        verbose (bool): Whether to show detailed progress messages. Default is True.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[bool, str]: (success status, detailed message)\n",
    "    \"\"\"\n",
    "    # Basic input validation\n",
    "    file_path = os.path.expanduser(file_path) # Expand relative paths to absolute paths\n",
    "    if not os.path.exists(file_path):\n",
    "        return False, f\"File not found: {file_path}\"\n",
    "    if not file_path.endswith('.csv'):\n",
    "        return False, \"Invalid file format. Must be CSV.\"\n",
    "    assert isinstance(save_metadata_to_disk, bool), \"save_to_disk must be a boolean.\"\n",
    "    assert isinstance(save_backup, bool), \"save_backup must be a boolean.\"\n",
    "    assert isinstance(save_log_file, bool), \"save_log_file must be a boolean.\"\n",
    "    assert isinstance(verbose, bool), \"verbose must be a boolean.\"\n",
    "\n",
    "    # Read metadata file\n",
    "    if verbose:\n",
    "        print(\"Reading metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(file_path, dtype=str)\n",
    "        metadata[\"publication_date\"] = pd.to_datetime(metadata[\"publication_date\"])\n",
    "        metadata = metadata.sort_values(by=\"publication_date\", ascending=False)\n",
    "        \n",
    "        if metadata.empty:\n",
    "            return False, \"Empty metadata file\"\n",
    "        \n",
    "        required_cols = ['oaid', 'cited_by_count', 'updated_date', 'doi_url']\n",
    "        if not all(col in metadata.columns for col in required_cols):\n",
    "            return False, f\"Missing required columns: {set(required_cols) - set(metadata.columns)}\"\n",
    "            \n",
    "        metadata_backup = deepcopy(metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading metadata file: {str(e)}\"\n",
    "\n",
    "    # Fetch works data from OpenAlex API\n",
    "    if verbose:\n",
    "        print(\"Calling OpenAlex API ...\")\n",
    "    try:\n",
    "        valid_ids = []\n",
    "        for _, row in metadata.iterrows():\n",
    "            oaid = str(row['oaid'])\n",
    "            if pd.notna(oaid):\n",
    "                oaid_clean = oaid.split('/')[-1] if '/' in oaid else oaid\n",
    "                valid_ids.append(oaid_clean)\n",
    "\n",
    "        if not valid_ids:\n",
    "            return False, \"No valid OpenAlex IDs found\"\n",
    "\n",
    "        works, failed_calls = get_works(\n",
    "            ids=valid_ids,\n",
    "            email=os.getenv(\"EMAIL\"),\n",
    "            select_fields=\"id,doi,cited_by_count,updated_date\",\n",
    "            show_progress=verbose\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return False, f\"API error: {str(e)}\"\n",
    "\n",
    "    # Update citation counts\n",
    "    updated_count = 0\n",
    "    errors = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Updating citation counts...\")\n",
    "    for idx, row in metadata.iterrows():\n",
    "        try:\n",
    "            oaid = str(row[\"oaid\"])\n",
    "            doi = row[\"doi_url\"]\n",
    "            current_citations = int(row[\"cited_by_count\"]) if pd.notna(row[\"cited_by_count\"]) else 0\n",
    "            \n",
    "            work = next((w for w in works if w[\"metadata\"][\"id\"] == oaid), None)\n",
    "            \n",
    "            if not work:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                new_citations = work[\"metadata\"][\"cited_by_count\"]\n",
    "            except TypeError as e:\n",
    "                if verbose:\n",
    "                    print(f\"TypeError: {e}\")\n",
    "                    print(f\"Work: {work}\")\n",
    "                    print(f\"Row type: {row['type']}\")\n",
    "                continue\n",
    "\n",
    "            if new_citations > current_citations:\n",
    "                if verbose:\n",
    "                    print(f\"Updating citations for OAID: {oaid} / DOI: {doi} from {current_citations} to {new_citations}\")\n",
    "                metadata.at[idx, 'cited_by_count'] = str(new_citations)\n",
    "                metadata.at[idx, 'updated_date'] = work[\"metadata\"][\"updated_date\"]\n",
    "                updated_count += 1\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Citation count for OAID: {oaid} / DOI: {doi} is up-to-date. Citation count: {current_citations}. Skipping...\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error processing {oaid}: {str(e)}\")\n",
    "            if verbose:\n",
    "                print(f\"Failed to update the cited_by_count for ID: {oaid}\")\n",
    "                print(e)\n",
    "            continue\n",
    "\n",
    "    # Save updates if any were made\n",
    "    if updated_count > 0:\n",
    "        if save_metadata_to_disk:\n",
    "            if save_backup:\n",
    "                try:\n",
    "                    if verbose:\n",
    "                        print(\"Saving a backup of the original metadata file...\")\n",
    "                    backup_timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "                    backup_path = file_path.replace(\".csv\", f\"_bkp-{backup_timestamp}.csv\")\n",
    "                    metadata_backup.to_csv(backup_path, index=False)\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error saving backup: {str(e)}\"\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Saving updated metadata to disk...\")\n",
    "            try:\n",
    "                metadata.to_csv(file_path, index=False)\n",
    "            except Exception as e:\n",
    "                return False, f\"Error saving updated metadata to disk: {str(e)}\"\n",
    "            \n",
    "            if save_log_file:\n",
    "                if verbose:\n",
    "                    print(\"Updating the log file...\")\n",
    "                try:\n",
    "                    log_data = {\n",
    "                        \"last_modified\": datetime.now().strftime('%Y-%m-%d'),\n",
    "                        \"status_message\": f\"Successfully updated citation counts for {updated_count} articles\",\n",
    "                    }\n",
    "                    with open(os.path.join(os.path.dirname(file_path), \"update-log.json\"), 'w') as f:\n",
    "                        json.dump(log_data, f, indent=2)\n",
    "                except Exception as e:\n",
    "                    return False, f\"Error updating log file: {str(e)}\"\n",
    "        \n",
    "            return True, f\"Successfully updated citation counts for {updated_count} articles and saved metadata to disk.\"\n",
    "        else: \n",
    "            return True, f\"Successfully updated citation counts for {updated_count} articles. No changes saved to disk.\"\n",
    "    else:\n",
    "        return True, \"No updates made. Citation counts were up-to-date.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../utils/website_utils.py\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def parse_data(works: List[Dict[str, Any]], exclude_errata: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the raw data from the OpenAlex API and create a DataFrame.\n",
    "\n",
    "    This function extracts relevant information from each work in the input list\n",
    "    and creates a DataFrame with specified columns. It also removes duplicates\n",
    "    based on PMID and filters out errata (if specified).\n",
    "\n",
    "    Args:\n",
    "        works (List[Dict[str, Any]]): A list of dictionaries, where each dictionary\n",
    "            contains metadata about a work.\n",
    "        exclude_errata (bool): Whether to exclude errata from the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing extracted and processed information\n",
    "        from the works.\n",
    "\n",
    "    Example:\n",
    "        >>> df_works = parse_data(works)\n",
    "        >>> df_works.head()\n",
    "\n",
    "    Note:\n",
    "        The function extracts the following information for each work:\n",
    "        - First author's last name\n",
    "        - Article title\n",
    "        - Journal name\n",
    "        - Publication year and date\n",
    "        - PMID, PMCID, and OpenAlex ID\n",
    "        - PDF URL (if available)\n",
    "        - DOI URL\n",
    "        - Citation count and URL\n",
    "        - Work type and Crossref type\n",
    "        - Updated date (from the API)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty list to store the extracted data, and iterate over the works data to extract relevant information\n",
    "    oa_data = []\n",
    "    for work in works:\n",
    "        metadata = work[\"metadata\"]\n",
    "        first_author_last_name = metadata[\"authorships\"][0][\"author\"][\"display_name\"].split(\" \")[-1]\n",
    "        article_title = metadata[\"title\"]\n",
    "        journal = metadata[\"primary_location\"][\"source\"][\"display_name\"]\n",
    "        publication_year = str(metadata[\"publication_year\"])\n",
    "        publication_date = metadata[\"publication_date\"]\n",
    "        if publication_date:\n",
    "            try:\n",
    "                publication_date = pd.to_datetime(publication_date).strftime('%Y-%m-%d')\n",
    "            except ValueError:\n",
    "                pass # If the date can't be parsed, keep the original string\n",
    "        pmid = metadata[\"ids\"].get(\"pmid\", \"\").split(\"/\")[-1] # To remove the url prefix\n",
    "        pmcid = metadata[\"ids\"].get(\"pmcid\")\n",
    "        if pmcid is not None:\n",
    "            pmcid = pmcid.split(\"/\")[-1] # To remove the url prefix\n",
    "        else: \n",
    "            pmcid = \"\" # To replace None with an empty string\n",
    "        oaid = metadata[\"id\"]\n",
    "        try:\n",
    "            pdf_url = metadata.get(\"best_oa_location\", {}).get(\"pdf_url\", \"not available\")\n",
    "        except AttributeError:\n",
    "            pdf_url = \"not available\"\n",
    "        if pdf_url is None:\n",
    "            pdf_url = \"not available\"\n",
    "        doi_url = metadata[\"doi\"]\n",
    "        cited_by_count = str(metadata[\"cited_by_count\"])\n",
    "        cited_by_ui_url = metadata[\"cited_by_api_url\"].replace(\"api.openalex.org\", \"openalex.org\")\n",
    "        work_type = metadata.get(\"type\")\n",
    "        type_crossref = metadata.get(\"type_crossref\")\n",
    "        updated_date = metadata.get(\"updated_date\")\n",
    "\n",
    "        oa_data.append([\n",
    "            first_author_last_name, article_title, journal, publication_year,\n",
    "            publication_date, pmid, pmcid, oaid, pdf_url, doi_url,\n",
    "            cited_by_count, cited_by_ui_url, work_type, type_crossref, updated_date\n",
    "        ])\n",
    "\n",
    "    columns = [\n",
    "        'first_author_last_name', 'article_title', 'journal',\n",
    "        'publication_year', 'publication_date', 'pmid', 'pmcid', 'oaid',\n",
    "        'pdf_url', 'doi_url', 'cited_by_count', 'cited_by_ui_url', 'type',\n",
    "        'type_crossref', 'updated_date'\n",
    "    ]\n",
    "\n",
    "    # Create a DataFrame with the specified columns\n",
    "    df_works = pd.DataFrame(oa_data, columns=columns, dtype=str)\n",
    "    df_works = df_works.drop_duplicates(subset=[\"pmid\"])\n",
    "    if exclude_errata:\n",
    "        df_works = df_works[df_works[\"type\"] != \"erratum\"]\n",
    "\n",
    "    # Parse the publication date as a datetime object with the format 'YYYY-MM-DD'\n",
    "    df_works[\"publication_date\"] = pd.to_datetime(df_works[\"publication_date\"], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Sort the DataFrame by publication date in descending order\n",
    "    df_works = df_works.sort_values(by=\"publication_date\", ascending=False)\n",
    "    df_works.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a ../utils/website_utils.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "def append_metadata(metadata_file_path: str, pmid_file_path: str, exclude_errata: bool = True, verbose: bool = True) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Append metadata for missing PMIDs to an existing metadata file.\n",
    "\n",
    "    Args:\n",
    "        metadata_file_path (str): Path to CSV file containing existing metadata.\n",
    "        pmid_file_path (str): Path to file containing list of PMIDs.\n",
    "        exclude_errata (bool): Whether to exclude errata from the metadata.\n",
    "        verbose (bool): Whether to show verbose messages during the process.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing a boolean indicating if any updates were made, and a message string with details.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert metadata_file_path.endswith(\".csv\"), \"Invalid file format. Please provide a CSV file.\"\n",
    "    assert os.path.exists(metadata_file_path), \"Metadata file not found.\"\n",
    "    assert pmid_file_path.endswith(\".txt\"), \"Invalid file format. Please provide a TXT file.\"\n",
    "    assert os.path.exists(pmid_file_path), \"PMID file not found.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean.\"\n",
    "\n",
    "    # Read existing metadata\n",
    "    if verbose: print(\"Reading the existing metadata file...\")\n",
    "    try:\n",
    "        metadata = pd.read_csv(metadata_file_path, dtype=str)\n",
    "        metadata_bkp = deepcopy(metadata) # Make a deepcopy of the DataFrame to save a backup\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the metadata file: {e}\")\n",
    "        return False, f\"An error occurred while reading the metadata file: {e}\"\n",
    "\n",
    "    # Read PMIDs from file\n",
    "    if verbose: print(\"Reading the PMID file...\")\n",
    "    try:\n",
    "        with open(pmid_file_path, 'r') as f:\n",
    "            pmids = set(line.strip() for line in f)\n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"An error occurred while reading the PMID file: {e}\")\n",
    "        return False, f\"An error occurred while reading the PMID file: {e}\"\n",
    "\n",
    "    # Find missing PMIDs\n",
    "    if verbose: print(\"Searching for new PMIDs not in the metadata...\")\n",
    "    existing_pmids: Set[str] = set(metadata[\"pmid\"])\n",
    "    new_pmids: Set[str] = pmids - existing_pmids\n",
    "    new_pmids_str = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "    if verbose: print(f\"Found {len(new_pmids)} new PMID(s): {new_pmids_str}.\")\n",
    "    if len(new_pmids) == 0:\n",
    "        return False, \"No new PMIDs found.\"\n",
    "    else:\n",
    "        try:\n",
    "            # Make API calls to get the missing PMIDs\n",
    "            select_fields: str = (\n",
    "                \"id,title,doi,primary_location,authorships,publication_year,\"\n",
    "                \"publication_date,ids,best_oa_location,cited_by_count,\"\n",
    "                \"cited_by_api_url,type,type_crossref,updated_date\"\n",
    "            )\n",
    "            new_articles, failed_calls = get_works(\n",
    "                ids=list(new_pmids),\n",
    "                email=os.environ.get(\"EMAIL\"),\n",
    "                select_fields=select_fields,\n",
    "                show_progress=verbose\n",
    "            )\n",
    "            if verbose: print(f\"API calls completed. Failed calls: {len(failed_calls)}\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while fetching works data from the API: {e}\")\n",
    "            return False, f\"An error occurred while fetching works data from the API: {e}\"\n",
    "\n",
    "        # Parse the data for new articles\n",
    "        if verbose: print(\"Parsing the data for new articles...\")\n",
    "        try:\n",
    "            df_new_articles = parse_data(new_articles, exclude_errata=exclude_errata)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while parsing the data for new articles: {e}\")\n",
    "            return False, f\"An error occurred while parsing the data for new articles: {e}\"\n",
    "        \n",
    "        if df_new_articles.empty:\n",
    "            if exclude_errata:\n",
    "                if verbose: print(\"No new articles found (Errata excluded).\")\n",
    "                return False, \"No new articles found (Errata excluded).\"\n",
    "            else:\n",
    "                if verbose: print(\"No new articles found.\")\n",
    "                return False, \"No new articles found.\"\n",
    "        else:\n",
    "            # Append the new articles to the existing metadata\n",
    "            new_pmids = set(df_new_articles[\"pmid\"])\n",
    "            new_pmids = \", \".join(new_pmids) # Convert to a string with comma-separated values\n",
    "            if verbose: print(f\"Appending {len(df_new_articles)} new article(s) with PMID(s) {new_pmids} to the existing metadata...\")\n",
    "            try:\n",
    "                metadata = pd.concat([df_new_articles, metadata], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"An error occurred while appending the new articles to the existing metadata: {e}\")\n",
    "                return False, f\"An error occurred while appending the new articles to the existing metadata: {e}\"\n",
    "\n",
    "            # Save the updated metadata to a CSV file\n",
    "            if verbose: print(\"Saving the updated metadata to a CSV file...\")\n",
    "            metadata.to_csv(metadata_file_path, index=False)\n",
    "            if verbose: print(\"Saving a backup file to disk...\")\n",
    "            bkp_file_path = metadata_file_path.replace(\".csv\", f\"_bkp-{datetime.now().strftime('%Y%m%d-%Hh%Mm')}.csv\")\n",
    "            metadata_bkp.to_csv(bkp_file_path, index=False)\n",
    "            if verbose: print(\"Metadata updated successfully.\")\n",
    "\n",
    "            # Get the path to the log file from the metadata file path\n",
    "            log_file_path = os.path.join(os.path.dirname(metadata_file_path), \"update-log.json\")\n",
    "            \n",
    "            # Update the log file\n",
    "            try:\n",
    "                if verbose: print(\"Updating the log file...\")\n",
    "                with open(log_file_path, \"r\") as f:\n",
    "                    update_log = json.load(f)\n",
    "                current_date = datetime.now().strftime(\"%Y-%m-%d\")    \n",
    "                update_log[\"last_modified\"] = current_date # Expected format: {\"last_modified\": \"2024-08-06\"}\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump(update_log, f)\n",
    "                if verbose: print(f\"Log file updated successfully.\")\n",
    "            except Exception as e:\n",
    "                if verbose: print(f\"Error updating log file: {e}. Creating a new log file...\")\n",
    "                current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "                with open(log_file_path, \"w\") as f:\n",
    "                    json.dump({\"last_modified\": current_date}, f)\n",
    "                if verbose: print(f\"New log file created successfully.\")\n",
    "\n",
    "            return True, f\"Appended {len(df_new_articles)} article(s) and saved file to {metadata_file_path}. Backup saved as {bkp_file_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../utils/main.py\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.expanduser(\"../utils\"))\n",
    "from website_utils import update_citations, append_metadata\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Manage website metadata and citations.\")\n",
    "    \n",
    "    # Main operation group\n",
    "    group = parser.add_mutually_exclusive_group(required=True)\n",
    "    group.add_argument(\"--update-citations\", action=\"store_true\", help=\"Update citation counts in the metadata file\")\n",
    "    group.add_argument(\"--append-metadata\", action=\"store_true\", help=\"Append metadata for missing PMIDs\")\n",
    "    group.add_argument(\"--update-and-append\", action=\"store_true\", help=\"Perform both update and append operations\")\n",
    "\n",
    "    # Common arguments\n",
    "    parser.add_argument(\"directory\", type=str, help=\"Directory containing the metadata, log, and PMID files\")\n",
    "    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode (no verbose output)\")\n",
    "    parser.add_argument(\"--include-errata\", action=\"store_true\", help=\"Include errata in the appended metadata\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Define file paths\n",
    "    metadata_file = os.path.join(args.directory, \"articles-metadata.csv\")\n",
    "    log_file = os.path.join(args.directory, \"update-log.json\")\n",
    "    pmid_file = os.path.join(args.directory, \"PMID-export.txt\")\n",
    "\n",
    "    # Validate file existence\n",
    "    if not os.path.exists(metadata_file):\n",
    "        parser.error(f\"Metadata file not found: {metadata_file}\")\n",
    "    if not os.path.exists(log_file):\n",
    "        parser.error(f\"Log file not found: {log_file}\")\n",
    "    if (args.append_metadata or args.update_and_append) and not os.path.exists(pmid_file):\n",
    "        parser.error(f\"PMID file not found: {pmid_file}\")\n",
    "\n",
    "    success_messages = []\n",
    "    error_messages = []\n",
    "\n",
    "    if args.update_citations or args.update_and_append:\n",
    "        success, message = update_citations(metadata_file, verbose=not args.quiet)\n",
    "        if success:\n",
    "            success_messages.append(f\"Update citations operation: {message}\")\n",
    "        else:\n",
    "            error_messages.append(f\"Update citations operation completed without saving new data: {message}\")\n",
    "\n",
    "    if args.append_metadata or args.update_and_append:\n",
    "        success, message = append_metadata(metadata_file, pmid_file, \n",
    "                                           exclude_errata=not args.include_errata, \n",
    "                                           verbose=not args.quiet)\n",
    "        if success:\n",
    "            success_messages.append(f\"Append metadata operation: {message}\")\n",
    "        else:\n",
    "            error_messages.append(f\"Append metadata operation completed without saving new data: {message}\")\n",
    "\n",
    "    # Print results\n",
    "    for message in success_messages:\n",
    "        print(message)\n",
    "    for message in error_messages:\n",
    "        print(message)\n",
    "\n",
    "    # Exit with error if any operation failed\n",
    "    if error_messages:\n",
    "        exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For execution from REPL / Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.expanduser(\"../utils\"))\n",
    "from website_utils import update_citations, parse_data, append_metadata\n",
    "from openalex_api_utils import get_works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to test metadata file\n",
    "%ls ~/GitHub/nicomarr.github.io/_py/test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test command line execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../_py/utils/main.py --update-citations ../../_py/test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the backup file in the test directory to remove the timestamp\n",
    "# This will overwrite the existing metadata file in the test directory for subsequent tests\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "test_dir_path = Path(\"~/GitHub/nicomarr.github.io/_py/test_data/\").expanduser()\n",
    "backup_files = sorted(test_dir_path.glob(\"articles-metadata_bkp-*.csv\"), key=lambda p: p.stat().st_mtime)\n",
    "try:\n",
    "    last_backup_file = backup_files[-1]\n",
    "    new_name = re.sub(r'_bkp-\\d{8}-\\d{6}', '', last_backup_file.stem) + '.csv'\n",
    "    last_backup_file.rename(last_backup_file.parent / new_name)\n",
    "    print(f\"Renamed {last_backup_file.name} to {new_name}\")\n",
    "except IndexError:\n",
    "    print(\"No backup files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for execution from command line in the root directory of the repository\n",
    "\n",
    "***Make sure to activate the virtual environment before running the commands and that the required packages are installed!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update citation counts\n",
    "To update citation counts in the `test_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-citations ./_py/test_data\n",
    "```\n",
    "To update citation counts in the `_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-citations ./_data\n",
    "```\n",
    "Or in quiet mode:\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-citations ./_data --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append metadata with new articles\n",
    "To append metadata in the `test_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --append-metadata ./_py/test_data\n",
    "```\n",
    "\n",
    "To append metadata in the `_data` directory:\n",
    "```sh\n",
    "python ./_py/utils/main.py --append-metadata ./_data\n",
    "```\n",
    "To include errata and run in quiet mode:\n",
    "```sh\n",
    "python ./_py/utils/main.py --append-metadata ./_data --quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform both update and append operations\n",
    "```sh\n",
    "python ./_py/utils/main.py --update-and-append ./_data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ~/GitHub/nicomarr.github.io/_py/utils/main.py --help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "public-tutorials-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
