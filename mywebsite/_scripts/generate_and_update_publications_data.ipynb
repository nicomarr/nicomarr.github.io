{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate & update metadata for the Publications page\n",
    "\n",
    "This notebook contains all the source code necessary to generate and update `articles-metadata.csv` in the `_data` directory using the OpenAlex API. This in turn is used by Jekyll as a [data file](\"https://jekyllrb.com/docs/datafiles\") to generate the Publications page via the Liquid templating language.\n",
    "\n",
    "The OpenAlex API docs can be found [here](\"https://docs.openalex.org/\"). Helpful tutorials explaining the basics of querying the OpenAlex API are found [here](\"https://github.com/ourresearch/openalex-api-tutorials/tree/main/notebooks/getting-started\").\n",
    "\n",
    "The only third party libraries required to run this notebook are [requests](\"https://requests.readthedocs.io/en/latest/api/\") and [pandas](\"https://pandas.pydata.org/\"). All other imports are part of the [Python standard library](\"https://docs.python.org/3/library/index.html\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile publications_page_utils.py\n",
    "########################################################################################\n",
    "# Utility functions for working with the OpenAlex API.\n",
    "# These functions help automatically generate and update the articles-metadata.csv and \n",
    "# update-log.json files in the _data directory, which are needed to serve the publications page.\n",
    "# This file is built from contents in notebook `generate_and_update_publications_data.ipynb`.\n",
    "# Version: 1.0.0\n",
    "# Dependencies: requests, pandas, json, os, datetime, time, tqdm, typing, IPython.display\n",
    "########################################################################################\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def search_works_by_author(author_id: str, email: str, verbose: bool=False) -> dict:\n",
    "    \"\"\"\n",
    "    Search for works by author ID using the OpenAlex API.\n",
    "\n",
    "    Args:\n",
    "        author_id (str): Identifier of the author to search for.\n",
    "        email (str): The email address to use for the query. Optional.\n",
    "        verbose (bool): Whether to print out additional information. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        dict: The search results from the OpenAlex database.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not author_id:\n",
    "        raise ValueError(\"Author ID is required.\")\n",
    "    if not author_id.startswith(\"a\"):\n",
    "        raise ValueError(\"Author ID must start with 'a'.\")\n",
    "\n",
    "    # If no email is provided, use an empty string\n",
    "    if not email:\n",
    "        email = \"\"\n",
    "\n",
    "    # Set the base URL and the API endpoint\n",
    "    base_url = \"https://api.openalex.org/\"\n",
    "    url = f\"{base_url}works?\"\n",
    "\n",
    "    params = {\n",
    "        \"mailto\": email,\n",
    "        \"filter\": f\"authorships.author.id:{author_id}\",\n",
    "        \"per-page\": 200,\n",
    "        \"select\": \"id,ids,doi,title,display_name,authorships,best_oa_location,primary_location,locations,publication_year,publication_date,biblio,open_access,topics,concepts,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    }\n",
    "    # filter options: abstract.search, display_name.search, fulltext.search, raw_affiliation_strings.search, title.search, title_and_abstract.search\n",
    "    # select option: id, doi, title, authorships, publication_year, publication_date, ids, language, primary_location, type, type_crossref, open_access, has_fulltext, cited_by_count, cited_by_percentile_year, biblio, primary_topic, topics, keywords, concepts, mesh, best_oa_location, sustainable_development_goals, referenced_works, related_works, ngrams_url, cited_by_api_url, counts_by_year, updated_date, created_date\n",
    "\n",
    "    # Initialize cursor\n",
    "    cursor = \"*\"\n",
    "\n",
    "    # Initialize the list to store all results and the number of API queries\n",
    "    api_query_count = 0\n",
    "    search_results = []\n",
    "\n",
    "    # Loop through pages\n",
    "    while cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Error:\", response.json())\n",
    "            break\n",
    "        current_page_results = response.json()[\"results\"]\n",
    "        for result in current_page_results:\n",
    "            # Store these results in the list we created before the loop we are currently in\n",
    "            search_results.append(result)\n",
    "        api_query_count += 1\n",
    "\n",
    "        # Update cursor using the `next_cursor` metadata field in the response\n",
    "        cursor = response.json()[\"meta\"][\"next_cursor\"]\n",
    "    if verbose:\n",
    "        print(f\"Done paging through results. Made {api_query_count} API queries, retrieved {len(search_results)} results.\")\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "# Example usage:\n",
    "# search_results = search_works_by_author(author_id, email)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a publications_page_utils.py\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "def search_works_by_pmid(pmids: list, email: str, show_progress: bool = False, verbose: bool = False) -> list:\n",
    "    \"\"\"\n",
    "    Get information about works from OpenAlex API. Works are scholarly documents like journal articles, books, datasets, and theses.\n",
    "    \n",
    "    Args:\n",
    "        pmids (list): List of PubMed IDs of works to get information about.\n",
    "        email (str): Email address to use in the API request. Optional but recommended.\n",
    "        show_progress (bool, optional): If True, displays a progress bar. Defaults to False.\n",
    "        verbose (bool, optional): If True, prints detailed status messages. Defaults to False. Disabled if show_progress is True.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing information about the works.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert isinstance(pmids, list), \"PMIDs must be provided as a list.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean value.\"\n",
    "\n",
    "    # Remove any duplicates from the list of PMIDs\n",
    "    pmids = list(set([pmid for pmid in pmids if pmid]))\n",
    "\n",
    "    # Remove any spaces from each item in the list of PMIDs\n",
    "    pmids = [pmid.strip() for pmid in pmids]\n",
    "\n",
    "    # If no email is provided, use an empty string\n",
    "    if not email:\n",
    "        email = \"\"\n",
    "    \n",
    "    # Initialize variables used for the API request and function\n",
    "    base_url = \"https://api.openalex.org/works/\"\n",
    "    params = {\n",
    "        \"mailto\": email,\n",
    "        \"select\": \"id,ids,doi,title,display_name,authorships,best_oa_location,primary_location,locations,publication_year,publication_date,biblio,open_access,topics,concepts,cited_by_count,cited_by_api_url,type,type_crossref,updated_date\",\n",
    "    }\n",
    "\n",
    "    # Initializer variables\n",
    "    search_results = []\n",
    "    iter_count = 0\n",
    "    now = datetime.now()  # Initialize 'now' variable\n",
    "\n",
    "    # Display a progress bar if show_progress is True\n",
    "    if show_progress:\n",
    "        iterable = tqdm(pmids, desc=\"Retrieving works\")\n",
    "        verbose = False  # Disable verbose mode if show_progress is True.\n",
    "    else:\n",
    "        iterable = pmids\n",
    "\n",
    "    # Iterate over each PMID in the list to retrieve information about the works.\n",
    "    for pmid in iterable:\n",
    "        # Initialize variables used for each iteration\n",
    "        response = None\n",
    "        data = None\n",
    "        url = None\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"---\")\n",
    "\n",
    "        # Handle API rate limit\n",
    "        if iter_count > 9:\n",
    "            time_delta = datetime.now() - now\n",
    "            if time_delta < timedelta(seconds=1):\n",
    "                remaining_time = 1 - time_delta.total_seconds()\n",
    "                if verbose:\n",
    "                    print(f\"Number of requests reached 10. Sleeping for {round(remaining_time, 3)} seconds...\")\n",
    "                time.sleep(remaining_time)\n",
    "                iter_count = 0\n",
    "                now = datetime.now()\n",
    "\n",
    "        # Construct the URL for the API call\n",
    "        url = f\"{base_url}pmid:{pmid}\"\n",
    "\n",
    "        # Retrieve data for the work using the OpenAlex API\n",
    "        try: \n",
    "            response = requests.get(url, params=params)    \n",
    "        except requests.RequestException as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while making an API call with PMID {pmid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Handle unsuccessful API calls    \n",
    "        if response.status_code != 200: \n",
    "            if verbose:\n",
    "                print(f\"API call for work with PMID {pmid} was not successful. Status code: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        # Continue if the API call was successful   \n",
    "        else:    \n",
    "            data = response.json()\n",
    "            if verbose:\n",
    "                print(f\"Successfully retrieved metadata for work with PMID {pmid}.\")\n",
    "\n",
    "        search_results.append(data)\n",
    "        iter_count += 1  # Increment the iteration count\n",
    "            \n",
    "    return search_results\n",
    "\n",
    "# Example usage:\n",
    "# search_results = search_works_by_pmid(pmids, email, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a publications_page_utils.py\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def process_raw_search_results(search_results: List[Dict[str, Any]], display_html: bool = False) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Format the search results retrieved from the OpenAlex API.\n",
    "\n",
    "    Args:\n",
    "        search_results (dict): List of dictionaries containing information about the works.\n",
    "        display_html (bool): Boolean flag to control HTML display. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List of processed article information.\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "\n",
    "    for work in search_results:\n",
    "        first_author_last_name = work[\"authorships\"][0][\"author\"][\"display_name\"].split(\" \")[-1]\n",
    "        title = str(work[\"title\"])\n",
    "        title = title.replace('\"', '')\n",
    "        publication_year = work[\"publication_year\"]\n",
    "        publication_date = work[\"publication_date\"]\n",
    "\n",
    "        # Get the journal name\n",
    "        journal = \"Unknown\"  # Default value\n",
    "        primary_location = work.get(\"primary_location\")\n",
    "        if primary_location is not None:\n",
    "            source = primary_location.get(\"source\")\n",
    "            if source is not None:\n",
    "                journal = source.get(\"display_name\", \"Unknown\")\n",
    "\n",
    "        # Get the number of citations and the URL to view the citing articles\n",
    "        cited_by_api_url = work[\"cited_by_api_url\"]\n",
    "        cited_by_ui_url = cited_by_api_url.replace(\"api.openalex.org\", \"openalex.org\")\n",
    "        cited_by_count = work[\"cited_by_count\"]\n",
    "\n",
    "        # Get DOI URL\n",
    "        doi = work.get(\"doi\")\n",
    "        doi_url = doi if doi and doi.startswith(\"http\") else \"not available\"\n",
    "\n",
    "        # Initialize pdf_url\n",
    "        _pdf_url = None\n",
    "        pdf_url = \"not available\"\n",
    "\n",
    "        # Get the PMID\n",
    "        pmid = work[\"ids\"].get(\"pmid\", \"\")\n",
    "        if pmid.startswith(\"http\"):\n",
    "            pmid = pmid.split(\"/\")[-1]\n",
    "\n",
    "        # Get OpenAlex ID\n",
    "        oaid = work[\"id\"]\n",
    "\n",
    "        # Get the PMCID and construct pdf_url\n",
    "        pmcid = work[\"ids\"].get(\"pmcid\", \"\")\n",
    "        if pmcid.startswith(\"http\"):\n",
    "            pmcid = pmcid.split(\"/\")[-1]\n",
    "            pdf_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/\"\n",
    "        elif pmcid.startswith(\"PMC\"):\n",
    "            pdf_url = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf\"\n",
    "\n",
    "        # Update the PDF URL from the best OA location\n",
    "        best_oa_location = work.get(\"best_oa_location\")\n",
    "        if best_oa_location is not None:\n",
    "            _pdf_url = best_oa_location.get(\"pdf_url\", None)\n",
    "            if _pdf_url is not None and _pdf_url.startswith(\"http\"):\n",
    "                pdf_url = _pdf_url\n",
    "\n",
    "        if display_html:\n",
    "            print(pmid, pmcid, oaid)\n",
    "            display(\n",
    "                HTML(f\"{first_author_last_name} <i>et al.</i> <b>{title}.</b> {journal} {publication_year}\"),\n",
    "                HTML(f\"\"\"\n",
    "                {\"<a href='\" + doi_url + \"' target='_blank'>Read article</a>\" if doi_url != \"not available\" else \"DOI not available\"}\n",
    "                | \n",
    "                {\"<a href='\" + pdf_url + \"' target='_blank'>Download PDF</a>\" if pdf_url != \"not available\" else \"PDF not available\"}\n",
    "                | \n",
    "                <a href='{cited_by_ui_url}'>Cited by</a>: {cited_by_count}\n",
    "                \"\"\".replace('\\n', ' ')),\n",
    "                HTML(\"<hr>\")\n",
    "            )\n",
    "\n",
    "        article = {\n",
    "            \"first_author_last_name\": first_author_last_name,\n",
    "            \"article_title\": title,\n",
    "            \"journal\": str(journal),\n",
    "            \"publication_year\": publication_year,\n",
    "            \"publication_date\": publication_date,\n",
    "            \"pmid\": str(pmid),\n",
    "            \"pmcid\": str(pmcid),\n",
    "            \"oaid\": str(oaid),\n",
    "            \"pdf_url\": str(pdf_url),\n",
    "            \"doi_url\": str(doi_url),\n",
    "            \"cited_by_count\": cited_by_count,\n",
    "            \"cited_by_ui_url\": str(cited_by_ui_url),\n",
    "            \"type\": work[\"type\"],\n",
    "            \"type_crossref\": work[\"type_crossref\"],\n",
    "            \"updated_date\": work[\"updated_date\"],\n",
    "        }\n",
    "        articles.append(article)\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Example usage:\n",
    "# processed_search_results = process_raw_search_results(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a publications_page_utils.py\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def save_results_to_csv(\n",
    "    articles: List[Dict[str, Any]], \n",
    "    output_path: str = \"../_data/articles-metadata.csv\", \n",
    "    exclude: List[str] = [], \n",
    "    is_journal_article: bool = True, \n",
    "    has_doi: bool = True, \n",
    "    has_pmid: bool = True, \n",
    "    sort_by_publication_date: bool = True, \n",
    "    verbose: bool = False,\n",
    "    return_df: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save the processed search results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        articles (list): List of processed article information.\n",
    "        output_path (str): Path to save the CSV file. Defaults to \"../_data/articles-metadata.csv\".\n",
    "        exclude (list): List of article IDs to exclude from the CSV file. Defaults to an empty list. Accepts OpenAlex IDs or PMIDs.\n",
    "        is_journal_article (bool): Whether to include only journal articles. Defaults to True.\n",
    "        has_doi (bool): Whether to include only articles with DOIs. Defaults to True.\n",
    "        has_pmid (bool): Whether to include only articles with PMIDs. Defaults to True.\n",
    "        sort_by_publication_date (bool): Whether to sort the articles by publication date. Defaults to True.\n",
    "        verbose (bool): Whether to print out additional information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the list of articles\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "    # Filter the DataFrame based on flags\n",
    "    if is_journal_article:\n",
    "        df = df[df[\"type_crossref\"] == \"journal-article\"]\n",
    "        df = df[~df[\"type\"].isin([\"erratum\", \"dataset\"])]\n",
    "\n",
    "    if has_doi:\n",
    "        df = df[~df[\"doi_url\"].isnull() & (df[\"doi_url\"] != \"\")]\n",
    "\n",
    "    if has_pmid:\n",
    "        df = df[~df[\"pmid\"].isnull() & (df[\"pmid\"] != \"\")]\n",
    "\n",
    "    # Remove works that are in the exclude list\n",
    "    df = df[~df[\"pmid\"].isin(exclude) & ~df[\"oaid\"].isin(exclude)]\n",
    "\n",
    "    # Sort the DataFrame by publication date\n",
    "    if sort_by_publication_date:\n",
    "        # Publication date is in the format \"YYYY-MM-DD\"; may need to convert to datetime for proper sorting\n",
    "        df.sort_values(\"publication_date\", ascending=False, inplace=True)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Check if DataFrame is empty before saving\n",
    "    if df.empty:\n",
    "        print(\"No articles to save after filtering.\")\n",
    "        return\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Save a json log file with a datetime timestamp of the metadata file to reflect the last update date on the bottom of the publications page\n",
    "    update_log(output_path) \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved data for {len(df)} articles to {output_path}\")\n",
    "\n",
    "    if return_df:\n",
    "        return df\n",
    "\n",
    "# Example usage:\n",
    "# save_results_to_csv(processed_search_results, output_path=\"../_data/articles-metadata.csv\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a publications_page_utils.py\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "def update_publications(\n",
    "    pmids: list,\n",
    "    email: str,\n",
    "    file_path: str = \"../_data/articles-metadata.csv\",\n",
    "    exclude: List[str] = [], \n",
    "    is_journal_article: bool = True, \n",
    "    has_doi: bool = True, \n",
    "    has_pmid: bool = True, \n",
    "    sort_by_publication_date: bool = True,\n",
    "    verbose: bool = False,\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Updates the publications by fetching new articles based on provided PMIDs.\n",
    "\n",
    "    Args:\n",
    "        pmids (list): List of PMIDs to check for new articles.\n",
    "        email (str): Email address for API requests.\n",
    "        file_path (str): Path to the CSV file containing existing articles.\n",
    "        exclude (List[str]): List of PMIDs or OAIDs to exclude from the results.\n",
    "        is_journal_article (bool): Flag to filter for journal articles.\n",
    "        has_doi (bool): Flag to filter for articles with a DOI.\n",
    "        has_pmid (bool): Flag to filter for articles with a PMID.\n",
    "        sort_by_publication_date (bool): Flag to sort the results by publication date.\n",
    "        verbose (bool): Flag to print additional information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the existing data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Determine which PMIDs are in the pmids list but not in the dataframe.pmid column\n",
    "    new_pmids = set(pmids) - set(df[\"pmid\"].astype(str))\n",
    "\n",
    "    # Call OpenAlex API to retrieve information about the new works\n",
    "    new_results = search_works_by_pmid(list(new_pmids), email, show_progress=True)\n",
    "\n",
    "    # Format the new results\n",
    "    new_results_formatted = process_raw_search_results(new_results, display_html=False)\n",
    "\n",
    "    # Cast the new results to a DataFrame\n",
    "    df_new = pd.DataFrame(new_results_formatted)\n",
    "\n",
    "    # Filter the DataFrame with the new results based on flags\n",
    "    if is_journal_article:\n",
    "        df_new = df_new[df_new[\"type_crossref\"] == \"journal-article\"]\n",
    "        df_new = df_new[~df_new[\"type\"].isin([\"erratum\", \"dataset\"])]\n",
    "\n",
    "    if has_doi:\n",
    "        df_new = df_new[~(df_new[\"doi_url\"].isnull() | (df_new[\"doi_url\"] == \"\"))]\n",
    "\n",
    "    if has_pmid:\n",
    "        df_new = df_new[~(df_new[\"pmid\"].isnull() | (df_new[\"pmid\"] == \"\"))]\n",
    "\n",
    "    # Remove works that are in the exclude list\n",
    "    df_new = df_new[~df_new[\"pmid\"].isin(exclude) & ~df_new[\"oaid\"].isin(exclude)]\n",
    "\n",
    "    # Check if the new DataFrame is empty\n",
    "    if df_new.empty:\n",
    "        if verbose:\n",
    "            print(\"No new articles to add.\")\n",
    "        return\n",
    "\n",
    "    # Append the new results to the existing DataFrame\n",
    "    df = pd.concat([df, df_new], ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame by publication date\n",
    "    if sort_by_publication_date:\n",
    "        df.sort_values(\"publication_date\", ascending=False, inplace=True)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the updated DataFrame to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    \n",
    "    # Update the json log file with a datetime timestamp of the metadata file to reflect the last update date on the bottom of the publications page\n",
    "    update_log(file_path) \n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Updated the data with {len(df_new)} new article(s).\")\n",
    "\n",
    "# Example usage:\n",
    "# update_publications(\n",
    "#     pmids=list(pmids),\n",
    "#     email=EMAIL,\n",
    "#     file_path=\"../_data/articles-metadata.csv\",\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a publications_page_utils.py\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def update_citation_counts(pmids: list, email: str, file_path: str = \"../_data/articles-metadata.csv\", verbose: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Update the citation counts for the articles in the CSV file.\n",
    "\n",
    "    Args:\n",
    "        pmids (list): List of PMIDs to update the citation counts for.\n",
    "        email (str): Email address to use in the API request. Optional but recommended.\n",
    "        file_path (str): Path to the CSV file containing the articles metadata. Defaults to \"../_data/articles-metadata.csv\".\n",
    "        verbose (bool): Whether to print out additional information. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    assert isinstance(pmids, list), \"PMIDs must be provided as a list.\"\n",
    "    assert isinstance(verbose, bool), \"Verbose must be a boolean value.\"\n",
    "\n",
    "    # If no email is provided, use an empty string\n",
    "    if not email:\n",
    "        email = \"\"\n",
    "    \n",
    "    # Initialize variables used for the API request and function\n",
    "    base_url = \"https://api.openalex.org/works/\"\n",
    "    params = {\n",
    "        \"mailto\": email,\n",
    "        \"select\": \"ids,cited_by_count,updated_date\",\n",
    "    }\n",
    "\n",
    "    # Initialize variables used for the API request and function\n",
    "    search_results = []\n",
    "    iter_count = 0\n",
    "    update_count = 0\n",
    "    now = datetime.now()  # Initialize 'now' variable\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Reading the existing data...\")\n",
    "\n",
    "    # Load the existing data\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Get the PMIDs from the DataFrame\n",
    "    pmids = df[\"pmid\"].tolist()\n",
    "\n",
    "    # Get the date and time when the publications data file was last updated\n",
    "    last_modification_datetime_of_pub_data = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "    if verbose:\n",
    "        print(f\"Publications data were last modified on {last_modification_datetime_of_pub_data}.\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Calling OpenAlex API to get updates for {len(pmids)} works...\")\n",
    "    for pmid in pmids:\n",
    "        # Initialize variables used for each iteration\n",
    "        response = None\n",
    "        data = None\n",
    "        url = None\n",
    "\n",
    "        # Handle API rate limit\n",
    "        if iter_count > 9:\n",
    "            time_delta = datetime.now() - now\n",
    "            if time_delta < timedelta(seconds=1):\n",
    "                remaining_time = 1 - time_delta.total_seconds()\n",
    "                if verbose:\n",
    "                    print(f\"Number of requests reached 10. Sleeping for {round(remaining_time, 3)} seconds...\")\n",
    "                time.sleep(remaining_time)\n",
    "                iter_count = 0\n",
    "                now = datetime.now()\n",
    "\n",
    "        # Construct the URL for the API call\n",
    "        url = f\"{base_url}pmid:{pmid}\"\n",
    "\n",
    "        # Retrieve data for the work using the OpenAlex API\n",
    "        try: \n",
    "            response = requests.get(url, params=params)    \n",
    "        except requests.RequestException as e:\n",
    "            if verbose:\n",
    "                print(f\"An error occurred while making an API call with PMID {pmid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Handle unsuccessful API calls    \n",
    "        if response.status_code != 200: \n",
    "            if verbose:\n",
    "                print(f\"API call for work with PMID {pmid} was not successful. Status code: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        # Continue if the API call was successful   \n",
    "        else:    \n",
    "            data = response.json()\n",
    "            if verbose:\n",
    "                print(f\"Successfully retrieved metadata for work with PMID {pmid}.\")\n",
    "\n",
    "        search_results.append(data)\n",
    "        iter_count += 1  # Increment the iteration count\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Finished calling the API. Processing the results...\")\n",
    "\n",
    "\n",
    "    # Check if the update_date for each search result is newer than the existing data\n",
    "    for work in search_results:\n",
    "        # Get the PMID and update date from the search result\n",
    "        _pmid = work[\"ids\"].get(\"pmid\", \"\")\n",
    "        if _pmid.startswith(\"http\"):\n",
    "            _pmid = _pmid.split(\"/\")[-1]\n",
    "        _update_date = work.get(\"updated_date\", \"\")\n",
    "\n",
    "        # Convert _update_date str to datetime format\n",
    "        # Example format of the _update_date str is \"2024-08-01T11:17:58.717683\"\n",
    "        _update_date = datetime.strptime(_update_date, \"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "\n",
    "        # Check if the updated date is newer than the existing data\n",
    "        if not _update_date > last_modification_datetime_of_pub_data:\n",
    "            if verbose:\n",
    "                print(f\"Data for PMID {_pmid} is up-to-date. Skipping the update.\")\n",
    "            continue\n",
    "        else:\n",
    "            # Get the citation count from the search result\n",
    "            _cited_by_count = work.get(\"cited_by_count\", \"\")\n",
    "\n",
    "            # Update the citation count in the DataFrame\n",
    "            if verbose:\n",
    "                print(f\"Data for PMID {_pmid} is outdated. Updating the citation count to {_cited_by_count}...\")\n",
    "            \n",
    "            df.loc[df[\"pmid\"] == _pmid, \"cited_by_count\"] = _cited_by_count\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Update of the citation count for PMID {_pmid} is complete.\")\n",
    "            update_count += 1\n",
    "    if verbose:\n",
    "        print(f\"Finished processing the results. Updated the citation counts for {update_count} articles.\")\n",
    "\n",
    "    # Save the updated DataFrame with the new citation counts to the CSV file if there were updates\n",
    "    if update_count > 0:\n",
    "        if verbose:\n",
    "            print(\"Saving the updated data to the CSV file...\")\n",
    "        df.to_csv(\"../_data/articles-metadata.csv\", index=False)\n",
    "        if verbose:\n",
    "            print(\"Data saved successfully.\")\n",
    "\n",
    "        # Update the log file with the new update timestamp to reflect the last update date on the bottom of the publications page\n",
    "        update_log(file_path) \n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No updates were made. Data file was not modified.\")\n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "\n",
    "# Example usage:\n",
    "# update_citation_counts(pmids, email, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The log file generated be the `update_log()` function below is used by Jekyll as a data file to display the `Last update`field on the Publications page. Therefore, this function is executed every time we run the `update_publications()` and/or the `update_citation_counts()` function to keep things in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a publications_page_utils.py\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def update_log(file_path: str = \"../_data/articles-metadata.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Update the log file with the last modification date of the articles metadata file.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the articles metadata file. Defaults to \"../_data/articles-metadata.csv\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get the last modification date of the articles metadata file\n",
    "    last_modified = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Create a dictionary to store the last modification date\n",
    "    log_data = {\n",
    "        \"last_modified\": last_modified\n",
    "    }\n",
    "\n",
    "    # Save the log data to a JSON file\n",
    "    with open(\"../_data/update-log.json\", \"w\") as file:\n",
    "        json.dump(log_data, file)\n",
    "\n",
    "# Example usage:\n",
    "# update_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from publications_page_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve article metadata and save data to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Query OpenAlex API using a list of PMIDs (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a list of PMIDs from a text file\n",
    "with open(\"../_data/PMID-export.txt\", \"r\") as file:\n",
    "    pmids = file.read().splitlines()\n",
    "print(\"No of IDs:\", len(pmids))\n",
    "print(pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = os.environ.get(\"EMAIL\") if \"EMAIL\" in os.environ else \"\"\n",
    "search_results = search_works_by_pmid(pmids, EMAIL, show_progress=True)\n",
    "formatted_results = process_raw_search_results(search_results, display_html=False)\n",
    "save_results_to_csv(formatted_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Query OpenAlex API using the OpenAplex author ID\n",
    "\n",
    "This may cause errors if the author ID is not correctly assigned to an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author_id = \"a5060691242\"\n",
    "# EMAIL = os.environ.get(\"EMAIL\") if \"EMAIL\" in os.environ else \"\"\n",
    "# search_results = search_works_by_author(author_id, EMAIL, verbose=False)\n",
    "# formatted_results = process_raw_search_results(search_results, display_html=False)\n",
    "# save_results_to_csv(formatted_results, output_path =\"../_data/articles-metadata-by-au.csv\",exclude=[\"https://openalex.org/W4390510186\", \"https://openalex.org/W4230282703\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata from disk and update newly added PMIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a list of PMIDs from a text file\n",
    "with open(\"../_data/PMID-export.txt\", \"r\") as file:\n",
    "    pmids = file.read().splitlines()\n",
    "print(\"No of IDs:\", len(pmids))\n",
    "print(pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = os.environ.get(\"EMAIL\") if \"EMAIL\" in os.environ else \"\"\n",
    "update_publications(\n",
    "    pmids=pmids,\n",
    "    email=EMAIL,\n",
    "    file_path=\"../_data/articles-metadata.csv\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update citation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL = os.environ.get(\"EMAIL\") if \"EMAIL\" in os.environ else \"\"\n",
    "update_citation_counts(pmids, EMAIL, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripts that can be executed from the CLI\n",
    "\n",
    "This is useful if automating the updates by GitHub actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile update_publications_list.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "update_publications_list.py\n",
    "\n",
    "This script updates the articles-metadata.csv file in the _data directory with new publications.\n",
    "It only adds new articles and does not update or overwrite existing metadata.\n",
    "\n",
    "Usage: python update_publications_list.py [--input INPUT_FILE] [--output OUTPUT_FILE] [--verbose]\n",
    "\n",
    "Dependencies: publications_page_utils.py, pandas, requests\n",
    "\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from publications_page_utils import update_publications\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Update publications list.\")\n",
    "    parser.add_argument(\"--input\", default=\"../_data/PMID-export.txt\", help=\"Path to input PMID file\")\n",
    "    parser.add_argument(\"--output\", default=\"../_data/articles-metadata.csv\", help=\"Path to output CSV file\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO if args.verbose else logging.WARNING)\n",
    "\n",
    "    try:\n",
    "        with open(args.input, \"r\") as file:\n",
    "            pmids = file.read().splitlines()\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file not found: {args.input}\")\n",
    "        return 1\n",
    "\n",
    "    email = os.environ.get(\"EMAIL\", \"\")\n",
    "    if not email:\n",
    "        logging.warning(\"EMAIL environment variable not set. Proceeding without email.\")\n",
    "\n",
    "    try:\n",
    "        update_publications(\n",
    "            pmids=pmids,\n",
    "            email=email,\n",
    "            file_path=args.output,\n",
    "            verbose=args.verbose,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error updating publications: {e}\")\n",
    "        return 1\n",
    "\n",
    "    logging.info(\"Publications list updated successfully.\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile update_citation_counts.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "update_citation_counts.py\n",
    "\n",
    "This script updates the citation counts for all articles in the articles-metadata.csv file in the _data directory.\n",
    "It only updates entries that have been modified in the OpenAlex database since the last modification date of the articles-metadata.csv file.\n",
    "\n",
    "Usage: python update_citation_counts.py [--input INPUT_FILE] [--output OUTPUT_FILE] [--pmid_file PMID_FILE] [--verbose]\n",
    "\n",
    "Dependencies: publications_page_utils.py, pandas, requests, tqdm\n",
    "\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "from publications_page_utils import update_citation_counts\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Update citation counts for publications.\")\n",
    "    parser.add_argument(\"--input\", default=\"../_data/articles-metadata.csv\", help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"--output\", default=\"../_data/articles-metadata.csv\", help=\"Path to output CSV file\")\n",
    "    parser.add_argument(\"--pmid_file\", default=\"../_data/PMID-export.txt\", help=\"Path to PMID file\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO if args.verbose else logging.WARNING)\n",
    "\n",
    "    # Check if the articles-metadata.csv file was modified within the last 24 hours\n",
    "    try:\n",
    "        last_modified = datetime.fromtimestamp(os.path.getmtime(args.input))\n",
    "        if (datetime.now() - last_modified) < timedelta(hours=24):\n",
    "            logging.warning(\"The articles-metadata.csv file was modified within the last 24 hours. Aborting the update.\")\n",
    "            return 1\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Input file not found: {args.input}\")\n",
    "        return 1\n",
    "\n",
    "    # Import the PMIDs from a text file\n",
    "    try:\n",
    "        with open(args.pmid_file, \"r\") as file:\n",
    "            pmids = file.read().splitlines()\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"PMID file not found: {args.pmid_file}\")\n",
    "        return 1\n",
    "\n",
    "    # Get the email address from the environment variables\n",
    "    email = os.environ.get(\"EMAIL\", \"\")\n",
    "    if not email:\n",
    "        logging.warning(\"EMAIL environment variable not set. Proceeding without email.\")\n",
    "\n",
    "    try:\n",
    "        update_citation_counts(\n",
    "            pmids=pmids,\n",
    "            email=email,\n",
    "            file_path=args.input,\n",
    "            verbose=args.verbose\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error updating citation counts: {e}\")\n",
    "        return 1\n",
    "\n",
    "    logging.info(\"Citation counts updated successfully.\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run update_publications_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run update_citation_counts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
